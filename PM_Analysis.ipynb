{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bbecdd5-5f59-4c62-91a7-618d2f8b7749",
   "metadata": {},
   "source": [
    "# Prelude: Using Jupyter Lab\n",
    "\n",
    "If you know all about Jupyter Lab already, skip to the [PM sensor data](#PM-Sensor-Data) section. If you have run this notebook before or an experienced Pandas user you might even want to jump [straight to the main action](#From-zero-to-plot).\n",
    "\n",
    "## The most important things to know\n",
    "... but you don't need to know it all at once (or at all). Read the first bit and skim over the rest if you feel like it. In case you get lost  or confused you can always come back to start of the notebook.\n",
    "\n",
    "If you are already familiar with Jupyter Lab you might want to skip to the [start of the actual example](#PM-Sensor-Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66db64-818b-4a3c-96d5-e98085835e08",
   "metadata": {},
   "source": [
    "### Running code\n",
    "Jupyter notebooks are based around the concept of a *cell*. A cell can be one or more lines of code or a long text like this one. When you click on something in this notebook, you'll see a blue bar on the left side, that highlights the current cell.\n",
    "\n",
    "To execute a cell hit `Shift + Enter`. This moves the focus to the cell below, making it easy to execute a notebook from top to bottom. There are more options, which you can find in the `Run` menu entry above, but this gets you really far.\n",
    "\n",
    "If the text starts looking weird somewhere, you probably opened a text cell and are in the Markdown edit mode now. Executing that text cell (the same way you execute a code cell) makes it look nice again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2b25a-af89-4b0e-bb1d-7aed493d8aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on me and run me with Shift + Enter\n",
    "print('execute me!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc61ad-52ad-4e07-ae97-4dd595938c99",
   "metadata": {},
   "source": [
    "### Tab completion and documentation access\n",
    "The other most useful things to know are:\n",
    "- `Tab` for completing what you are typing in a code cell\n",
    "- `Shift + Tab` for getting a description of the function or object (with the cursor inside or on the parenthesis for functions), or:\n",
    "- `Ctrl + i` (or by choosing \"Help\" -> \"Show Contextual Help\" from the top menu) you get a new window that will display the help text for whatever you click on. If you click on a variable it also show you its type and the contents (in abbreviated for if necessary). Drag that window to the side or bottom by clicking and holding tab name of the window, so that you can see the notebook at the same time.\n",
    "- `Ctrl + Shift + c` (or \"View\" -> \"Activate Command Palette\" in the top menu) lets you search for anything else and show the keyboard shortcuts for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd2d76-7462-4420-a00f-94776a9e7d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# position the curser after \"up\" and hit tab\n",
    "'complete me'.up\n",
    "# and don't forget to add () to the completion to actually execute the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2b649-f440-4309-8cf9-43ee792732fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the cursor to the end of the next line and hit tab\n",
    "'complete me'.\n",
    "# then select one of the functions and call it, e.g. title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec953152-6f15-497f-8db8-c2b3312fc30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the cursor inside ( ) and hit Shift + Tab\n",
    "round()\n",
    "# or look at me with the Contextual Help window (Shift + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f8df92-ffdc-42cc-b8cb-ee2c9519ca56",
   "metadata": {},
   "source": [
    "Small tips and tricks:\n",
    "* If you hit `Tab` right after a `.` behind a Python object and wait a bit, you can also search for completions that don't start with what you type, e.g. in the example above if you hit tab after typing `'hello'.` and type `up`, you'll get both `upper` and `isupper`. If you hit tab after `'hello'.up` it only completes it to `upper` for you.\n",
    "* The tab completion and also the help text does not always show up as expected. This most often occurs when you chain several function calls, but also in other situations. In this case a workaround that nearly always works is to assign the intermediate result to a (throwaway) variable and then get the completion or help by using this new variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e945c4b-4de8-435c-a5b5-b8e4daf3214e",
   "metadata": {},
   "source": [
    "### Moving around quickly\n",
    "\n",
    "You can open the table of contents for the notebook by clicking on the symbol with three lines (with small dots on the side) symbol in the left toolbar. You can move to a section by clicking on its title there. This section list is generated automatically from the Markdown headings, with the indentation (and optional numbering) based on the how many `#` are used for the heading.\n",
    "\n",
    "If you right click on the tab of the notebook, you can also select \"New View for Notebook\", which opens a second window for the same notebook. You can display them side by side and be at different positions in each of them and even run code or change text in either. This can be really handy for example two similar looking code cells that produce slightly different output or for comparing plots. By the way, you can also create a new window for a plot (or any other output, like a dataframe table), by right clicking on it as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd6fe94-8d39-4f17-a182-0176c2db91bc",
   "metadata": {},
   "source": [
    "### Python session and restarting things\n",
    "The last thing that might be important to know is what the number in the `[ ]` left of each code cell means. It shows how many code cells have been executed when that cell was executed most recently. If it is empty, you haven't executed it yet. This can be useful to figure out if you already executed a cell after updating another one.\n",
    "\n",
    "It is possible to restart your Python session. This session is called a `kernel` in Jupyter for rather technical reasons. You can restart the session under \"Kernel\" -> \"Restart Kernel\" (or click on the nearly closed circle symbol in the toolbar).\n",
    "\n",
    "Beware though: the code cell execution counts described above stick around from the last session until you execute the cell again in the new session, unless you \"Clear All Outputs\" as well when restarting the kernel or afterwards (but that also removes your plots and other outputs in the notebook from the previous sessions that might still be useful).\n",
    "\n",
    "Sometimes you also want to stop Python from doing something, e.g. because you made a mistake and what it now tries to do takes way too long. For that go to \"Kernel\" -> \"Interrupt Kernel\" (or click on the filled black square symbol in the toolbar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378aae7-11af-4c85-a9b3-f9e4ee3597b0",
   "metadata": {},
   "source": [
    "# PM Sensor Data\n",
    "Finally we get to the actual example of analyzing the [PM (Particulate Matter)](https://en.wikipedia.org/wiki/Particulates) measurements from a low cost sensor placed in Lustenau, Austria.\n",
    "\n",
    "After going through this example you might want to try going this notebook again with a [sensor closer to you](https://maps.sensor.community/#10/47.3426/9.8812) and adjusting the code as needed. For background information on the sensors or the whole citizen science project you can head over to [sensor.community](https://sensor.community/en/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e6f12-69ef-4c23-a453-569d2e08de14",
   "metadata": {},
   "source": [
    "### The fast lane\n",
    "If you are not interested in the data import and validation details (yet) or are already an experienced Pandas user and just want to see the code without explanation, you can skip straight to the [visualizing and inspecting the data](#From-zero-to-plot) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c0dfc-6bcb-42fd-807e-f42c01f46a25",
   "metadata": {},
   "source": [
    "## Standard imports and configuration\n",
    "\n",
    "A few lines of boilerplate code you see at the beginning of nearly every Python file for data analysis or machine learning. You can just copy it for now, all you need to know is that the packages will be available under the abbreviated name if you do it like this.\n",
    "\n",
    "At least the commonly used python package names are usually abbreviated to avoid always having to type the name over and over again. For pandas, numpy and pyplot basically everyone uses the same abbreviations, so you might as well get used to them. For nearly all matplotlib operations the `pyplot` component is used, so I wouldn't bother to abbreviate the main package name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3d33b-ba28-46bc-a84d-52aa34501dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750124a6-9021-4fbe-ae81-994b01b11654",
   "metadata": {},
   "source": [
    "The next cell is for configuring matplotlib to work well in the notebook. The first line you might need (if matplotlib uses a different plotting backend for some reason), but the others you can leave away.\n",
    "\n",
    "I often don't bother with them and the `retina` figure format I discovered just recently (but it does make the plots look much nicer). The default figure size is too small for my taste, so if I leave that line away I keep repeating `figsize=(x,y)` all over the place. If you copy some other boilerplate anyways, you can just include that as well. If you keep using the same values, you can also set it as new default values for all notebooks in a configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ac31e-07d0-47c5-8a90-1153b641a498",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats=['retina'] # for high resoultion images\n",
    "plt.rcParams['figure.figsize'] = (12,6) # default plot size - play with it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147ff43-9671-4d79-a04b-0a5bb03be1f9",
   "metadata": {},
   "source": [
    "## Loading all sensor data\n",
    "### Getting the data\n",
    "#### The fast and easy way (pre-prepared)\n",
    "Just use the data I already prepared for you. It is just all the CSV file for this sensor (one per day) combined into one CSV file. I then zipped it to make it a bit smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782154c9-5765-4a16-a653-b0c34bcf4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_file_path = 'data/sds011_4365_combined.csv.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83205c6a-6414-4a3f-9cc4-67a1c55780f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The slow and tedious way\n",
    "\n",
    "You can safely [skip this section](#Reading-the-data) or even skip to the [plotting action](#From-zero-to-plot) if you are not interested in how you might want to do this in a similar situation. Below is a high-level description of how one get all the data and the nitty-gritty details and a walkthrough is available as a separate notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c1ab7-a6c5-47e6-894e-595e07363233",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Generating the download URLs**\n",
    "\n",
    "The URLs for the CSV files all follow the same pattern. Apart from the sensor type name and the sensor id we also need to know the dates at which the sensor reported data. This list of date is not available (as far as I know), so we just need to try all dates from the first date in the archive.\n",
    "\n",
    "This list of dates can be generated easily with pandas using:\n",
    "```python\n",
    "pd.date_range(first_date, last_date, freq='D')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353e9ae0-5769-41c4-b1d9-07516a74f80e",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Downloading data from the generated URLs**\n",
    "There are many different ways to download the data. Choose whichever is most comfortable and suitable in your situation. We can:\n",
    "* use pandas directly, just passing the URL to the `pd.read_csv` function.\n",
    "* write all the URLs to a text file and then use another program (that you can call directly from Jupyter with `!program_name`) to download them all like `wget` on Linux\n",
    "* loop over all the URLs, then download the file and save the contents to a local file. The downloading and saving can be done with an external tool (like `curl`) or directly from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ca2e7-19bf-4098-afab-aed9eed12a8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### **Combining the data**\n",
    "* If we read the date into pandas dataframes we have to concatenate the dataframes into a single dataframe. For this we can use the `pd.concat` function.\n",
    "* If we downloaded all the original CSV files, we can either loop over all of them and:\n",
    "  - read them into Pandas dataframes and then concatenate them (see above)\n",
    "  - read them as text files, remove the header and write the rest of the file into one output csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6b218-80f0-40b1-8fa5-4315e3f71044",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "#### Step-by-step\n",
    "Reading all the data in and then checking if our assumptions are valid. You can also skip to doing it [all at once](#loading-data-all-at-once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d2017-7224-4906-a0fe-3d8db6e5f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d96498-4a1b-47b0-8dc7-2dca1d9178f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.read_csv(sensor_file_path, sep=';')\n",
    "data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f1ffb-7db3-4069-80d7-fdf0c9d15318",
   "metadata": {},
   "source": [
    "##### Checking if columns are empty and dropping useless data\n",
    "We can already apply what we about this csv file from what we figured out in the single day analysis. But let's quickly confirm our assumptions first. With `count` we can see how many non-empty values the columns contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc849d-2072-4411-8b4a-beff44b00427",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88478eed-fc47-4f10-abf4-fe1fcbb5743f",
   "metadata": {},
   "source": [
    "As expected, we can get rid of the columns without any values (`durP1`, `ratioP1`, `durP2`, `ratioP2`) as they don't provide any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be89296-2094-4c31-90c3-493e032d6309",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_empty = data_all.dropna(axis='columns', how='all')\n",
    "data_without_empty.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527c0ebb-3a2b-4abc-94df-1f66edf542ae",
   "metadata": {},
   "source": [
    "##### Checking how many unique values are in columns\n",
    "Let's check how many unique values are in the columns that are left. We know already that our `selected_columns` contain a lot of interesting values, so let's temporarily exclude them for this with the `drop` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dde297-f582-4de4-889a-a2ac50ef6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['timestamp', 'P1', 'P2']\n",
    "data_leftover = data_without_empty.drop(selected_columns, axis='columns')\n",
    "data_leftover.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b4cb6-3f35-4a98-8d6d-d95b94b5a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_leftover.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6337eb-c8d7-48e5-91cd-cfb4add648f5",
   "metadata": {},
   "source": [
    "##### Counting how often each value appears\n",
    "We could also look at how many value combinations for the remaining columns exist and how often these combinations appear. If we call `value_counts` on a data frame it gives you all unique rows and how often they appear (unless you restrict it to subset of the columns with the first parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd326f-195a-4589-888d-cd99e34a9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = data_leftover.value_counts()\n",
    "all_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fed6ce-5f79-4739-a832-c59fa8d5b987",
   "metadata": {},
   "source": [
    "More commonly `value_counts` is used on a single column (or on a *Series* in Pandas terms). This is especially useful with string columns, e.g. columns containing country names or in this case sensor type names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88179afa-46b8-4c04-a9d1-28f6a3d54f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_leftover['sensor_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87057d-b19c-4915-842e-2f98bddfb66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_leftover['lon'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6337b5-5f39-4ba0-804d-6ae6e5dfd2bf",
   "metadata": {},
   "source": [
    "The coordinate longitude data changed slightly over time for some reason. Due to privacy considerations the coordinates in the sensor.community data are not exact anyways (rounded to a resolution of about 100m). Maybe they changed their rounding algorithm a bit at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd7fd1-4762-462b-b470-615b3dbced73",
   "metadata": {},
   "source": [
    "<a id=\"loading-data-all-at-once\"></a>\n",
    "#### All at once\n",
    "We can save memory and speed up the reading a bit by telling Pandas to use only the columns we are interested in with the `usecols` parameter.\n",
    "\n",
    "We can parse the datetime columns right away with `parse_dates`. Be careful to pass it a *list* of column names as it doesn't accept just a single name as a string (unlike many other Pandas methods). According to the help text it should automatically try to parse all datetime columns if you pass it `True`, but in my limited experience this didn't work.\n",
    "\n",
    "There is a **lot** more parameters available for `read_csv`. I recommend to have a short look at it (with `Shift + Tab` or better yet the contextual help with `Ctrl + i` if you haven't opened it already), just to get a bit of an idea how powerful it is. For German speakers `decimal` and maybe `thousands` could be needed rather often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f462894-4134-4216-b59d-65c0b8a66db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['timestamp', 'P1', 'P2']\n",
    "data_selected = pd.read_csv(sensor_file_path, sep=';',\n",
    "                            usecols=selected_columns,\n",
    "                            parse_dates=['timestamp'])\n",
    "data_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa15e38-7741-4f16-ba41-1a141079b27f",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "Let's first shorten the variable name as we'll use it a lot. If we want, we can `copy` the original dataframe. This can be useful if we might modify the data frame under this new name, but want to keep the original data around, e.g. to get back to it (without doing everything all over again).\n",
    "\n",
    "We also make the datetime column become our new row index. Datetime indices are very handy, as this gives us easier access to time ranges, make selecting dates faster and we can leave away the column name for the x-axis when we use Pandas plotting functions for time series plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffb75f0-476b-445b-a5ae-a05d8680c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_selected.copy() # copy is optional\n",
    "data = data.set_index('timestamp')\n",
    "print(data.columns)\n",
    "data.columns = ['PM10', 'PM2.5']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e02e2f-ad67-4afb-8b71-7e2bc5ef5d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1459af1-ffa8-438e-977b-df42e6300de5",
   "metadata": {},
   "source": [
    "## Validating the data\n",
    "### Datetime index\n",
    "\n",
    "Having a sorted index speeds up some operations and prevents misleading results if we select date ranges. Our index looks sorted, but let's check to be on the save side.\n",
    "\n",
    "The name for checking if an index (or column) is sorted is not very intuitive. Instead of something like *is_sorted* it is called `is_monotonic`. In case you are not familiar with the term, it is mostly used in University level math (or at least this is where I first encountered as a non-native speaker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e94bcb-d1b6-49b4-8e0f-cdddd527d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.is_monotonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab44fae-433c-400a-a34a-a855ba385c5a",
   "metadata": {},
   "source": [
    "#### Detour: checking why the datetime is not sorted\n",
    "You can safely skip to [Sorting the datetime index](#Sorting-the-datetime-index) if you are not interested.\n",
    "\n",
    "A bit of a surprise, it is not sorted, although it very much looks sorted.\n",
    "\n",
    "We can just sort the index and be done with it. You can skip the exploration below and do just that, but often looking into why one assumption is false leads to discovering other false assumptions or peculiarities in the data.\n",
    "\n",
    "Still with me? Then let's dig a bit into where and why this assumption fails. We start of by looking at the time span between two consecutive entries in the timespan data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cbab7-4848-4e62-b1a7-599faf5255eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff = data.index.to_series().diff(1)\n",
    "time_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c588e5-eb58-4db3-8dcf-65c087698c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851c974-29b6-4456-9864-8928c406decb",
   "metadata": {},
   "source": [
    "Okay, the very minimum and maximum are quite different than the rest. Let's see if this applies only to the most extrem values or if it is not that uncommon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c16f5-689a-4d4b-a05b-5f4c56b12a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff.describe(percentiles=[0.001, .01, .1, .25, .5, .75, .9, .99, .999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992f5904-b412-4d37-b152-da5bce601849",
   "metadata": {
    "tags": []
   },
   "source": [
    "The vast majority of the data was recorded roughly every 2 1/2 minutes. The larger gaps are rather rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3df43-7d9c-411b-b3a0-e03c946c7718",
   "metadata": {
    "tags": [
     "interlude",
     "pandas"
    ]
   },
   "source": [
    "##### Interlude: boolean vectors and counting True entries\n",
    "To see how many values are actually negative, we use a bit of a trick. The boolean vectors we get as a result of comparisons can also be treated as integers with 1 for `True` and 0 for `False`. Hence, to get the count of the `True` entries, we can just sum all the numbers together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c5b34f-82ab-427b-a78b-8bd61e6e4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff < pd.Timedelta(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235a0b7-96c8-4e69-a4b3-538fb1bbb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "(time_diff < pd.Timedelta(0)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a969db-ee28-4fe5-8d7c-d1cdf1e5a8c0",
   "metadata": {},
   "source": [
    "So, just one position where an earlier timestamp follows a later timestamp.\n",
    "\n",
    "To see which entry that actually is, we can use the boolean vector in our dataframe selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b2493-ce00-4797-aafc-6384b122956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[time_diff < pd.Timedelta(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377ffdfc-7690-48bc-ba28-3ded785e443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2018-04-02']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91bf151-6dd2-40ef-a907-1fc99e17f356",
   "metadata": {},
   "source": [
    "Let's get the row numbers by switching back to the standard row index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e5712-7a75-45c3-9969-53673d73be16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b730a-54f0-45c9-b63b-b5993e990382",
   "metadata": {
    "tags": [
     "interlude",
     "pandas"
    ]
   },
   "source": [
    "##### Interlude: selecting data with df.query()\n",
    "\n",
    "Selecting data with boolean conditions like this `df[df['column'] == 10]` can get annoying quickly, especially if you have multiple conditions and it is not possible to do it this way if you are working with a temporary dataframe that you haven't assigned to a variable yet.\n",
    "\n",
    "Using multiple conditions with boolean vectors can actually easily trip one up. Due to the order in which the operators like `==` and `&` are evaluated, we have to put parenthesis around the conditions to connect them with `&` (and) or  `|` (or). E.g.\n",
    "```python\n",
    "df[(df['column'] > 10) & (df['column'] < 20)]\n",
    "```\n",
    "\n",
    "If we forget the parenthesis, depending on the exact conditions we'll get an error or even worse, we don't get an error, but also not the result we intended!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a187241b-954a-4043-a2af-f2d5e8ab52c9",
   "metadata": {},
   "source": [
    "An alternative way to select data is with `df.query('column == 10')`. You pass the function a string in which you can use the column names directly and also use number or string literals. If you include strings though you need to quote them still, but with the two different quote styles in Python it is not that hard: `df.query('sensor_type == \"sds011\"')`\n",
    "\n",
    "Multiple conditions are easy with this syntax as well:\n",
    "```python\n",
    "df.query('10 <= column_name < 20')\n",
    "df.query('a == \"passed\" and status_code > 10')\n",
    "```\n",
    "\n",
    "One limitation though of the query syntax is that the column name must be a valid python variable name. For example, we couldn't query our `PM2.5` column with it as dots are not allowed in variable names (and neither are spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0922f8-bf15-419d-8941-0b5b49a308b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query('PM10 > 800')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf61b400-f51d-4b92-8f10-488e70769167",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index().query(\"timestamp == '2018-04-02 00:01:02'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a7f1f3-15ed-4c0b-8922-b357c70f5f17",
   "metadata": {},
   "source": [
    "With `df.iloc` instead of `df.loc` we can access the rows by their row number (0-based) instead of their labels. If your index also uses integers but doesn't match the row numbers, you have to be careful with which one you use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43baea5b-6912-4a17-aa60-ca0d65722964",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[148451:148457]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6435f-ab02-463f-9001-24a6d092a4f4",
   "metadata": {},
   "source": [
    "We suddenly jump from 2 April midnight to the start of the same day. Let's see what happens at the end of the previous day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d88fb7-4d94-4623-baa8-c3eb28f5bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index().query('timestamp > \"2018-04-01 23:55\"').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb9ffd-8239-4701-a55c-1dc7cb45ff31",
   "metadata": {},
   "source": [
    "Hmm, it jumps from 1st of April midnight to just before noon on the 2nd. We are getting closer to the mystery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35abc8e-8107-4cc1-9dd5-800959fcdd8f",
   "metadata": {
    "tags": [
     "interlude",
     "pandas"
    ]
   },
   "source": [
    "##### Interlude: [making Heads or Tails of](https://idioms.thefreedictionary.com/make+heads+or+tails+of) our timestamp mystery\n",
    "To look at what happens at the start of the next day, 3rd of April, and a bit before that (where we don't know what to expect) we can do the following:\n",
    "\n",
    "We select all data up to the end time we know (3rd April, after midnight) and then look at the last few entries before it with `df.tail(n)`, which shows `n` rows from the end, i.e. the opposite of what `head` is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b0c923-36be-4553-8faa-7765beb7b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index().query('timestamp < \"2018-04-03 00:10\"').tail(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca8003-70e5-4471-acd0-bc8f12bf8e2e",
   "metadata": {},
   "source": [
    "Looks like we have the following situation:\n",
    "* 1st April (normally)\n",
    "* 2nd April 11:50ish - midnight\n",
    "* 2nd April midnight to before 11:50\n",
    "* 3rd April (normally)\n",
    "\n",
    "When we look at the [original CSV file for that day](https://archive.sensor.community/2018-04-02/2018-04-02_sds011_sensor_4365.csv) we see that the timestamps are already messed up there. If we look at other sensors, we see the same pattern on that day, so it very much looks like the archive server was mixing up the row order on that day for some reason."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80825d-6c5b-4ce2-b180-042f97e5cf1c",
   "metadata": {},
   "source": [
    "#### Sorting the datetime index\n",
    "Luckily for us the solution is much shorter than the investigation was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b8853-aec8-47a0-91a6-a034dc1cab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = data.sort_index()\n",
    "data.index.is_monotonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69e2b3-29f0-40e7-9678-f32424a790ff",
   "metadata": {},
   "source": [
    "### Time zone handling\n",
    "\n",
    "The sensor data does not include any time zone information and in the archive description it is not mentioned either.\n",
    "\n",
    "So, what time zone do we have? Is it the local time? If so, does it switch between summer and winter time?\n",
    "\n",
    "In some forum comments it is mentioned that the datetime is using the [UTC, the Coordinated Universal Time](https://en.wikipedia.org/wiki/Coordinated_Universal_Time), so the offset to the local time in Austria is 1 hour in the winter and 2 hours in the summer.\n",
    "\n",
    "Working with time zones can be really painful, especially if one needs to coordinate between different formats. Luckily, Pandas is aware of time zones and does most of the work for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c91ade1-2b52-41d8-8abb-72c40078023d",
   "metadata": {},
   "source": [
    "#### Adding time zone information\n",
    "\n",
    "If we have datetimes without any time zone information, we first need to tell Pandas which time zone was used with `tz_localize`. Once this is done, we can convert it to whatever time zone we want with `tz_convert` (and if you forget the first step it won't let you do the second one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e09c1c-bfdd-4bf3-96cc-5f6ee0544c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tz_localize('UTC') # adds +00:00 to the timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4571e036-c0d3-4e0a-9b66-0733474141dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.tz_localize('UTC').tz_convert('Europe/Vienna')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3686c77-a87f-470f-b615-872fc55460c4",
   "metadata": {},
   "source": [
    "Switching between summer and winter time happens at the end of October. Let's see if Pandas handled it properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fbc50-97aa-4a37-b84e-85b2977cb6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2020-10-24':'2020-11-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba625c-3865-4b09-9e9d-cf3ff7a26994",
   "metadata": {},
   "source": [
    "Looks good, +02:00 in summer, +01:00 in winter.\n",
    "\n",
    "What time zone should be used for the analysis is actually not that clear cut. By looking at the local time we will likely preserve more the social aspects in the data (e.g. people commuting at the same clock time) and if we don't use the summer time change we should preserve more the natural aspects (e.g. sunrise time and how that might affect wind patterns).\n",
    "\n",
    "It could actually be interesting to look for PM pattern changes around the switching dates between winter and summer time. Let's stick with the local time for now, as we probably don't have enough data so far for a meaningful analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2867a16-95f9-4c34-a0df-4008431ea463",
   "metadata": {},
   "source": [
    "### All at once\n",
    "\n",
    "Let's do all the little changes we made above in one step. Otherwise it might happen, that we forget one or mix up the order at some point. It's also a nice review and closer to what an experienced Pandas user would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cdf86b-821b-4c58-974b-be4fbf1bb72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_selected.copy() # make a copy of the original data (optional)\n",
    "data = data.set_index('timestamp') # to access the data by datetime\n",
    "data.columns = ['PM10', 'PM2.5'] # rename columns from \"P1\", \"P2\"\n",
    "data = data.sort_index() # fix a small problem in the order of the datetime info\n",
    "data = data.tz_localize('UTC').tz_convert('Europe/Vienna') # use local time\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c365dbc3-47c4-4a8f-ad0a-35543b25fd5f",
   "metadata": {},
   "source": [
    "## Visualizing and inspecting the data\n",
    "### From zero to plot\n",
    "In case you jumped to this section or messed up your dataframes, below is the code for loading all data and get it into the shape for plotting. If you executed all the relevant stuff so far, you don't need to execute this cell.\n",
    "\n",
    "The code is hidden by default as we have seen all before in detail. If you want to see it anyways, click on the three dots or \"View\" -> \"Expand Selected Code\" from the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac9946-cd6d-419d-a0b9-f945713bc21c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats=['retina'] # for high resoultion images\n",
    "plt.rcParams['figure.figsize'] = (12,6) # default plot size - play with it\n",
    "\n",
    "sensor_file_path = 'data/sds011_4365_combined.csv.zip'\n",
    "selected_columns = ['timestamp', 'P1', 'P2']\n",
    "data_selected = pd.read_csv(sensor_file_path, sep=';',\n",
    "    usecols=selected_columns, parse_dates=['timestamp'])\n",
    "\n",
    "data = data_selected.copy().set_index('timestamp').sort_index()\n",
    "data = data.tz_localize('UTC').tz_convert('Europe/Vienna')\n",
    "data.columns = ['PM10', 'PM2.5']\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d139a85d-af73-4057-9074-5513073127be",
   "metadata": {},
   "source": [
    "### Exploring the values over time\n",
    "Now we are finally all prepared to look at the data. I usually start off by plotting it (or calculating summary statistics with `describe`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530c7ee5-ebfd-443f-89ac-ea6c7973ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot() ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ae5ed-fce5-4270-9e7e-6a2efa8156d2",
   "metadata": {},
   "source": [
    "That's a good overview of what data we have available. We can see range and can already spot some gaps in the data, but we can't see any details. Let's zoom in a bit.\n",
    "\n",
    "#### Selecting time ranges\n",
    "Selecting a date or time range is a breeze in Pandas with datetime indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9106950-1a2f-4d9d-9df6-eaaad938c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2020'].plot() ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcaf13e-1626-4990-b1bc-1c904bd44219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2021-05'].plot() ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866a7d22-df46-441b-98a1-79fae781c3cb",
   "metadata": {},
   "source": [
    "You might have noticed that the end date was included fully. Unlike other slices in Python and Pandas, datetime slices include the end of the range as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b324dd-b62f-4183-b4d8-dbe757ac7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2021-05':'2021-05-07'].plot() ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a26ae-5205-45b4-9f7b-2b20e4f02bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2021-04-30':'2021-05-02'].plot() ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2125941-a784-4df7-868d-30ab70008dbc",
   "metadata": {},
   "source": [
    "Let's zoom out a bit again and cut off the highest spikes, so we see more of the actual pattern and make the curves a bit transparent so that we can see the $PM_{10}$ values hidden behind the $PM_{2.5}$ values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06105a-51a3-42b0-98bd-bbda14f627ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2020'].plot(ylim=(0, 200), alpha=0.8) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7ba01-545d-4c6d-aa14-6c787a941ac1",
   "metadata": {
    "tags": [
     "interlude",
     "widget"
    ]
   },
   "source": [
    "#### Interlude: Interactively selecting the date ranges to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bd2141-3b3a-4586-95e7-e0cdc4684078",
   "metadata": {},
   "source": [
    "It is a lot of data to look at. Instead of keeping changing the date selection string in the code, we can do this more easily with widgets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532f778-ac0a-4b00-b45f-89be06865f63",
   "metadata": {},
   "source": [
    "To do that, we need to create the plotting function for it first, but if you want to just see the interactive widget in action, you can [jump to it](#interactive-timeplot). We want to be able to plot an offset with a fixed number of days. With the strings we have been using so far, this is a bit difficult (we would need to consider how many days are in a month,...).\n",
    "\n",
    "Let's start with creating our starting date first from variables. Python has a couple of ways for formatting strings, but the easiest is putting an `f` before the opening quote and then writing the variable names (or Python expressions) inside curly braces within the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956ccc83-9546-4ed8-9a18-5f536b1c53cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "month = 2\n",
    "day = 17\n",
    "f'{year}-{month}-{day}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549c083-86e6-48d9-a537-3e368950313c",
   "metadata": {},
   "source": [
    "This string we can pass to Python for creating the datetime object for us.\n",
    "\n",
    "*Side note: in any kind of production or polished code you wouldn't want to do this by using strings, but that is what we know so far. You would rather do something like this:*\n",
    "```python\n",
    "from datetime import datetime # built-in Python module\n",
    "start_time = datetime(year, month, day, tzinfo=data.index.tz)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f55d8-fadb-4061-8b10-bc5b270935f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(f'{year}-{month}-{day}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b859f5c8-9577-4726-9056-17266ef4cfd4",
   "metadata": {},
   "source": [
    "The end time we can get with Pandas offsets. This can actually do much more complicated offset than days (e.g. business month begin or Easter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e865a-efda-419e-82a0-be8cea9d2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.offsets.Day() * 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ec651-0a4a-441b-a2e0-9978ba1a1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.to_datetime(f'{year}-{month}-{day}')\n",
    "end_time = start_time + pd.offsets.Day() * 14\n",
    "end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6781b944-4f71-42f3-8a18-05d59499f2ba",
   "metadata": {},
   "source": [
    "When we try to select data with it, we get a slightly scary looking, but actually rather informative warning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9774d-49c0-4335-9f5f-0037a14c6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[start_time:end_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fe720a-1113-45a5-a3d7-f88621289d73",
   "metadata": {},
   "source": [
    "Seems like Pandas is stricter when the datetime range is selected with datetime objects instead of strings. This does make some sense, as the datetime object are more likely to be created programmatically or come from another data source that might use a different time zone, whereas the string form is mostly used interactively.\n",
    "\n",
    "We already know how to do that, so let's try it. We don't have to convert between time zones, as we can just choose which time zone our date selection should have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb9cd8-8bd2-4c6b-901e-93cc6c27344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time.tz_localize('Europe/Vienna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278bbe5-c587-483d-a13a-d3cb1691ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = pd.to_datetime(f'{year}-{month}-{day}')\n",
    "start_time = start_time.tz_localize('Europe/Vienna')\n",
    "end_time = start_time + pd.offsets.Day() * 14\n",
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7278ce1b-9a54-4683-94cf-91de9db97162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[start_time:end_time].plot(alpha=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232bda8-f293-4877-890c-32a8768c9303",
   "metadata": {},
   "source": [
    "Now we just put that code into a function. All we really need to do for that is give it a name with `def` and indent our code lines by hitting the tab key, like this:\n",
    "```python\n",
    "def function_name(parameter1, parameter2):\n",
    "    do_something_with(paramter1, parameter2)\n",
    "```\n",
    "\n",
    "It is possible to provide default values for the parameters. This way we can leave away some or all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913528a-4c3d-4d15-abd0-9b5f34b8d39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pm(year=2021, month=5, day=1, days_to_show=14):\n",
    "    start_time = pd.to_datetime(f'{year}-{month}-{day}').tz_localize('Europe/Vienna')\n",
    "    df = data[['PM10', 'PM2.5']] # in case we add other columns later\n",
    "    df.loc[start_time:start_time+pd.offsets.Day()*days_to_show].plot(ylim=0)\n",
    "    plt.show() # to make sure interactive output updates properly\n",
    "\n",
    "plot_pm()\n",
    "plot_pm(2020)\n",
    "plot_pm(month=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77083552-8818-4fee-9af9-9148d14855ac",
   "metadata": {},
   "source": [
    "##### Interactive plot of PM over time\n",
    "With the `plot_pm` function we just created above we are now all set for creating our interactive widget.\n",
    "\n",
    "The ipywidgets module should come with Jupyter. If not remove the `#` to uncomment the second cell below and execute it. This will download and install the package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a2202-3755-4a01-9e36-bc62f5091d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84f2c2-1a9e-4aac-b94d-c6ad779fad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d52404-607d-4df7-908a-8a8cad42e184",
   "metadata": {},
   "source": [
    "To add interactivity we just tell `interact` which function to use and here it makes sense to also tell it the value ranges we want to accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41150f3e-5dee-48d5-89f7-4984322497c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_pm, year=(2017, 2021), month=(1, 12), day=(0, 31),\n",
    "         days_to_show=(1, 60)) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbcc9c2-1bec-45f0-b784-fc96ee69e4dd",
   "metadata": {},
   "source": [
    "There are a lot of short spikes in the data and generally it looks quite noisy. The time resolution of the data is also higher than needed, if we want to look at many days at once. To make easier to extract information from it we can add some (optional) smoothing to our interactive pot\n",
    "\n",
    "First, we smooth the data a bit by first creating fixed size time bins. Additionally, we create a version that calculates a rolling average on top of that. The [`resample`](#resampling) and [`rolling`](#rolling) functions will be explained and shown in more detail in later sections.\n",
    "\n",
    "To select which version of the data we want to plot, we use make use of a dictionary. A dictionary is similar to a list, but instead of just being able to access the value by the position, we can access it by some kind of key. The keys are often strings but can be nearly anything. Dictionaries are used a lot in Python, even for very simple things, like our use of it here. We could write the same thing easily with `if` and else if (called `elif` in Python), but it is a bit shorter this way (especially if we end up adding more options).\n",
    "\n",
    "They even have a special syntax for creating them. Instead of the `dict()` used below, we could also write it as:\n",
    "```python\n",
    "data_versions = {'raw': data, 'smooth1': data_fixed_frequency, 'smooth2': data_rolling_average}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e271ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line if scipy is missing\n",
    "#%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c1a82-9a44-4900-bdae-e02990f68b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fixed_frequency = data[['PM10', 'PM2.5']].resample('10min').mean()\n",
    "data_rolling_average = data_fixed_frequency.rolling(13, center=True,\n",
    "                                                    win_type='gaussian').mean(std=3)\n",
    "\n",
    "data_versions = dict(raw=data, smooth1=data_fixed_frequency, smooth2=data_rolling_average)\n",
    "print(f'Available options: {data_versions.keys()}')\n",
    "\n",
    "def plot_pm_smoothed(year=2021, month=5, day=1, days_to_show=14, version='smooth2'):\n",
    "    data_to_use = data_versions[version]\n",
    "    data_to_use = data_to_use[['PM10', 'PM2.5']] # in case we add columns later\n",
    "    start_time = pd.to_datetime(f'{year}-{month}-{day}').tz_localize('Europe/Vienna')\n",
    "    data_to_use.loc[start_time:start_time+pd.offsets.Day()*days_to_show].plot(ylim=0)\n",
    "    plt.title(version)\n",
    "    plt.show() # to make sure interactive output updates properly\n",
    "\n",
    "plot_pm_smoothed(month=6, days_to_show=4)\n",
    "plot_pm_smoothed(month=6, days_to_show=4, version='smooth1')\n",
    "plot_pm_smoothed(month=6, days_to_show=4, version='raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22461ebb-54af-4353-9a84-0fabc9140e1e",
   "metadata": {},
   "source": [
    "Quite a difference between the different versions of the data. You might also have noticed that Pandas formats the $y$ axis differently in the smoothed versions. It does that as soon as your datetime index has a fixed known frequency (either created by binning / downsampling with `resample` or if you have by just letting Pandas know about it and fill in any missing time slot with null values with `asfreq`.\n",
    "\n",
    "Let's add the version selection to our interactive widget, so we can flip between them on the spot, e.g. to check if the raw data is suspicious. For that we just need to tell interact what values are possible (with a list or something similar like the dictionary keys we use here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d3c87-a961-4448-966c-ac6623aae68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_pm_smoothed, year=(2017, 2021), month=(1, 12), day=(0, 31),\n",
    "         days_to_show=(1, 60), version=data_versions.keys()) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5190477-6ec7-4583-9f07-50d15bf4e642",
   "metadata": {
    "tags": [
     "interlude",
     "pandas",
     "plotting"
    ]
   },
   "source": [
    "#### Interlude: Scatter plot\n",
    "If we look at the data over a long time period, there are just so many data points and short-term variations in it that a line plot is not the ideal way to visualize it. One alternative to the smoothing we did above is to plot each data point with a small dot in a scatter plot, especially if you are not familiar with the data (quality) yet. Smoothing can make some things, like for example erroneous data, hard to spot as we are not looking at the raw data anymore.\n",
    "\n",
    "Pandas *scatter plots* require an explicit column name for the x-axis (and it needs to be a column, not an index) and they also don't allow you to plot more than one column on the y-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400937c-3827-4938-add0-0142701d90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2020'].reset_index().plot.scatter('timestamp', 'PM10')\n",
    "data.loc['2020'].reset_index().plot.scatter('timestamp', 'PM2.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a69b9-077b-4dfb-b14c-b5c970eae073",
   "metadata": {},
   "source": [
    "With this many data points the dots overlap each other strongly as they are too big. We can improve this situation by using a different marker, `.` instead of the default  `o` (small letter O) and making the dots slightly transparent with the `alpha` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955ab59-3232-4d02-9970-a125b30e23f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2020'].reset_index().plot.scatter(\n",
    "    'timestamp', 'PM10', ylim=(0, 200), alpha=0.3, marker='.')\n",
    "data.loc['2020'].reset_index().plot.scatter(\n",
    "    'timestamp', 'PM2.5', ylim=(0, 200), alpha=0.3, marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9857335-d3dc-477a-8772-87cd644124ab",
   "metadata": {},
   "source": [
    "We can plot both variables in the same plot by telling the second plot to use the \"axes\" (the drawing area) from the first plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21985587-3b1b-473c-99d5-f78a1beaac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data.loc['2020'].reset_index().plot.scatter('timestamp', 'PM10',\n",
    "    ylim=(0, 200), alpha=0.1, marker='.')\n",
    "data.loc['2020'].reset_index().plot.scatter(\n",
    "    'timestamp', 'PM2.5', ylim=(0, 200), alpha=0.1, marker='.',\n",
    "    ax=ax, color='C1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b08c28f-3255-4286-b251-91844f99a2f4",
   "metadata": {},
   "source": [
    "#### Tip: Quick scatter plots with Pandas\n",
    "Here is a small trick: we can achieve the same thing more easily with the normal plot \n",
    "function. We just need to make the lines invisible with `linewidth=0` (or shorter with `lw=0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd943b-7865-47c8-9801-78f405e0269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2020'].plot(ylim=(0, 200), alpha=0.1, marker='.', linewidth=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b2a2fd-650c-4c62-a5f1-e610eb863676",
   "metadata": {},
   "source": [
    "We can also plot them separatly if we feel like it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dceec-1d04-4016-b70b-05996aac92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2020'].plot(ylim=(0, 100), alpha=0.1, marker='.', lw=0, subplots=True) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b904bb7c-3316-48a1-9d5e-6c2018a7c314",
   "metadata": {},
   "source": [
    "If you want to get closer to the two scatter plots above, we can give them both the same color and make the plots a bit taller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78122b2-05bd-4eb4-9193-3923121a0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2020'].plot(ylim=(0, 100), alpha=0.1, marker='.', lw=0, subplots=True,\n",
    "                      color='C0', figsize=(12,12)) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96c622-e11d-47c3-ade5-9f3aba5deec8",
   "metadata": {},
   "source": [
    "### Exploring the value distribution\n",
    "Now that we have looked a bit how the measurements change with time, it is time (no pun intended) to look at measurements themself in more detail.\n",
    "\n",
    "We can get useful summary statistics with `describe()` or plot the value distribution in histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cd629-2af1-4e94-862a-785c0fb2ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stats = data.describe()\n",
    "data_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e9cc7-622a-4988-a9d7-1dd7e1ad0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist() ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30501e24-aed9-4198-9874-d0e4de2b929b",
   "metadata": {},
   "source": [
    "With our value distribution the default histogram parameters are really not useful. Let's do something abou that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd221d7-5041-4ecc-a7fc-e8c5e706e283",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(log=True) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70559e7b-e0d7-456a-a73b-3e1cee4da11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(log=True, bins=100) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d19092-2be1-46bb-8315-1e365e5956da",
   "metadata": {},
   "source": [
    "#### Relationship between PM2.5 and PM10 values\n",
    "We have more than one measurement point per time and they measure similar things, so let's have a look at how they relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30bb88d-1df2-4320-8d9b-9a844c2bd169",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter('PM2.5', 'PM10') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e53c5d-c1cb-42ee-9fec-50dbbcdffad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['PM10'] < 250].plot.scatter('PM2.5', 'PM10', marker='.', alpha=0.3) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41061e-6047-4ddb-af3a-b87f38a5a240",
   "metadata": {},
   "source": [
    "The bottom of the cloud is cut off quite sharply. Looking at it more closely it is around where $PM_{2.5}$ and $PM_{10}$ have roughly same value.\n",
    "\n",
    "Hypothesis: this cut-off is not a measurement artifact. Instead it is due to $PM_{10}$ (particulates $< 10 \\mu g$) containing by definition all $PM_{2.5}$ particulates ($< 2.5\\mu g$). \n",
    "\n",
    "Let's add a line with $x == y$ to the plot. To do this we just need to define the start and end point, e.g. $0$ and $125$ and use the same start and end point for the $x$ and $y$ axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31d80e-70af-449f-a4c7-174dee8d44f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['PM10'] < 250].plot.scatter('PM2.5', 'PM10', marker='.', alpha=0.3)\n",
    "plt.plot([0, 125], [0, 125], color='orange') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd9f22-d3e9-44ee-914d-522544cbb1eb",
   "metadata": {},
   "source": [
    "Looks promising, but we should zoom in a bit more on the main cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736cda6-4f85-44ee-9808-0b31d36c699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlimits = 0, 40\n",
    "data[data['PM2.5'] < xlimits[1]].plot.scatter('PM2.5', 'PM10', marker='.', alpha=0.3)\n",
    "plt.plot(xlimits, xlimits, color='orange') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517be6e2-c4f5-4787-8dd6-b7002e0e1a21",
   "metadata": {},
   "source": [
    "Instead of selecting which values we want to show we can use all entries from the dataframe and just set the plot boundaries to the area we are interested in with xlim and ylim.\n",
    "\n",
    "This works well if you set the limits for both the $x$ and $y$ axis explicitly. However, if you just to limit one axis to a known range, the auto-scaling of the other axis will be based on all of the data, not just the ones corresponding to the limits you set on the other axis. This can result in the other axis having a too large value range. If that bothers either set the other limits based on what you see in the plot explicitly as well or go back to selecting just a subset of the data from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c58aef-1d02-4684-91b5-40649fb26ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlimits = 0, 40\n",
    "ax = data.plot.scatter('PM2.5', 'PM10', xlim=xlimits, ylim=(0, 120),\n",
    "                       marker='.', alpha=0.3)\n",
    "ax.plot(xlimits, xlimits, color='orange', alpha=0.3) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb4b0a-012a-4e9e-a0ff-b142a8532a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlimits = (0, 4)\n",
    "ax = data.plot.scatter('PM2.5', 'PM10', xlim=xlimits, ylim=(0, 12),\n",
    "                       marker='.', alpha=0.3)\n",
    "ax.plot(xlimits, xlimits, color='orange', alpha=0.3, lw=3) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95211cda-9f57-44fa-8327-578a274d5efe",
   "metadata": {},
   "source": [
    "I think we are close enough. The stripes we see are probably due to the limited resolution of the $PM_{2.5}$ sensor or something strange happening with the binning in the sensor (as there *are* a few data points in between the stripes, just way less).\n",
    "\n",
    "A few $PM_{10}$ data points are smaller than $PM_{2.5}$, likely due to measurement inaccuracy. However, this implies that $PM_{10} \\ge PM_{2.5}$ is probably not enforced in a post-processing step, but due to actual measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce3486-5093-4d7f-9ea4-5859efc188ba",
   "metadata": {},
   "source": [
    "#### PM10 particles that are larger than PM2.5\n",
    "Maybe it makes more sense to just look at $PM_{10}$ particulates that are larger than $2.5 \\mu g$ when comparing it with $PM_{2.5}$, as the smaller ones are already contained in the latter measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4947d9b-32f4-4b8a-8b6d-59d3269e0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PM10-2.5'] = data['PM10'] - data['PM2.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327d48d-d759-4435-8a01-4d11f8687618",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['PM10'] < 1000].plot.scatter('PM2.5', 'PM10-2.5', marker='.', alpha=0.3) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11228132-0af1-49e2-968d-d91c58150ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['PM2.5'] < 100].plot.scatter('PM2.5', 'PM10-2.5', marker='.', alpha=0.3,\n",
    "                                       ylim=(-1, 200)) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ddf1e-9197-4d46-8a7a-77a35a5abde8",
   "metadata": {},
   "source": [
    "So far, our plots have been far wider then taller, which lead to a noticeably smaller scale on the $y$ axis. This can lead to misleading visual impressions, so let's try to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef920ec3-b0a1-43ca-beeb-8a369246856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data[data['PM2.5'] < 100].plot.scatter('PM2.5', 'PM10-2.5', grid=True,\n",
    "                                            marker='.', alpha=0.3, figsize=(12,12))\n",
    "ax.set_ylim(-1, 200)\n",
    "ax.set_yticks(range(0, 201, 10))\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9b870-6654-4ecd-98fe-1dc83e102ebc",
   "metadata": {},
   "source": [
    "### Aggregating data over longer time periods\n",
    "#### Differences between years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb6cd1-b2fb-4932-abb1-35109e6ae887",
   "metadata": {
    "tags": [
     "interlude",
     "pandas"
    ]
   },
   "source": [
    "<a id=\"groupby\"></a>\n",
    "##### Interlude: Grouping subsets of data with groupby\n",
    "Often we want to compare different subsets of the data with each other. Pandas makes this rather easy with the `groupby` function. What it does is to select the subsets for you, based on the criteria you give it.\n",
    "\n",
    "Afterwards you optionally select which columns you want to work on and you select an operation you want to do on each of the subgroups. This is usually an aggregation of some kind, like taking the mean, but you could also do some transformation of the data, like subtracting the group mean from the values in each group separately.\n",
    "\n",
    "Usually the grouping is done on categorical data in one of the columns, like for example sensor id or sensor type name if we had data from more than one sensor. We don't have anything like this here, however, datetime columns (or indices) allow you to easily pull out a part of the datetime information (like year or hour):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25fce3-9323-49ed-92b6-482392e55456",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a999c6-b1b4-4a9d-beb2-e6840d810a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_year = data.groupby(data.index.year)\n",
    "grouped_by_year.ngroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66c27f-dc2a-4def-944c-4ec5f5393443",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_year.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e8d87-5b24-48eb-9fc7-76fbeeddb5b1",
   "metadata": {},
   "source": [
    "If the aggregation we choose return more than one column, we suddenly have two levels of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f990835a-9c98-464c-a255-a9c49564f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_stats = grouped_by_year.describe()\n",
    "year_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f8aaa-9ef7-46cd-ad06-8dcee9895751",
   "metadata": {},
   "source": [
    "If you look at each of the individual original columns separately, it is not that hard to work with it, because as soon as we select one of the outer / upper column values we get back a dataframe like we are used to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a86c3-da0a-49cd-ae7c-6cd3d2afe9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_stats['PM2.5']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f7dfa-c5c9-463e-a387-9ba46ef0816d",
   "metadata": {},
   "source": [
    "However, selecting one (or more) column names from the lower level for all of the upper levels (like getting the mean for all of our PM values) is a bit trickier. We will discuss that a bit later [here](#hierarchical-column-index-part1) and [here](#hierarchical-column-index-part2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640e9dc-7510-4020-9ebd-5b6239413493",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = ['min', '25%', '50%', 'mean', '75%']\n",
    "ax = year_stats['PM2.5'].plot.bar(y=plot_cols) # max is too high!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d3cc16-839e-4ffb-abf9-d42aa50592f6",
   "metadata": {
    "tags": [
     "interlude",
     "python"
    ]
   },
   "source": [
    "###### Mini-Interlude: Getting the position of a value in a list: enumerate\n",
    "We can add the same statistic measures over all years to the plot for a better comparison. For this we use `enumerate`, which takes lists (or other \"iterables\") and returns a list of tuples `(i, original_value)`, where `i` is the count of values from the original list up to that time (which is also its index in the original list).\n",
    "\n",
    "To get to see the values we need the turn it into a `list` (or loop over it), as enumerate is evaluated lazily, so just returning the values one by one as you ask for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8033e-7b2a-4583-a62d-aba4d1626c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(plot_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b041d3-cba3-46c4-b4f5-bd41b727a994",
   "metadata": {},
   "source": [
    "The matplotlib standard colors of the active style are accessible with `C0`, `C1`,... For the default style we have used so far, the first color `C0` is blue and the second `C1` is orange, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db754037-7ab0-42d0-bb29-801656da8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = ['min', '25%', '50%', 'mean', '75%']\n",
    "ax = year_stats['PM2.5'].plot.bar(y=plot_cols) # max is too high!\n",
    "for i, stastistic_name in enumerate(plot_cols):\n",
    "    ax.axhline(data_stats.loc[stastistic_name, 'PM2.5'], color=f'C{i}', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0c1217-272b-4a29-a7d5-ef1945574681",
   "metadata": {},
   "source": [
    "Looks good already, but the labels on the $y$ axis are spaced a bit too far apart to estimate the values well. We can tell matplotlib at which $y$ values to place the labels with `ax.set_yticks` (or `plt.yticks`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9166a86e-843c-44bf-bebf-dde40f29d3a4",
   "metadata": {
    "tags": [
     "interlude",
     "python"
    ]
   },
   "source": [
    "###### Mini-Interlude: Generating a list of regularly spaced numbers: range\n",
    "I'm too lazy to type out the numbers from 1 to 13, but I don't have to. The built-in Python function `range` gives as back a range of integers. If we just give it a number $n$ it gives as the numbers from 0 to $n-1$. This might take a bit getting used to, but the upper end of ranges or slices in Python are not included.\n",
    "\n",
    "However, this actually works quite well with $0$-based counting in Python: if we call `range(n)` we do get `n` values (just each one lower than you might expect). This also works well if you do need the numbers as an index into a list or so. Although explicit indexing is usually avoided in Python, sometimes you do need it and the typical way to do a C-style for-loop is this:\n",
    "```python\n",
    "for i in range(len(a_list_or_so)):\n",
    "    do_something_with(a_list_or_so[i])\n",
    "```\n",
    "\n",
    "If you want to create ranges to use with Pandas, there is a very similar function in `numpy` called `arange`. It works the same way but returns a numpy array (and is not evaluated lazily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dea600-08e3-43f7-bbaa-fe295158a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96911f40-0618-4d97-8f6b-9112cbeabdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(3, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea721987-6923-48d5-9a32-df98efb27b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(9, 2, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3858d-22d1-4998-b5ef-7cf75c786efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = ['min', '25%', '50%', 'mean', '75%']\n",
    "ax = year_stats['PM2.5'].plot.bar(y=plot_cols) # max is too high!\n",
    "for i, stastistic_name in enumerate(plot_cols):\n",
    "    ax.axhline(data_stats.loc[stastistic_name, 'PM2.5'], color=f'C{i}', alpha=0.5)\n",
    "\n",
    "upper_ylimit = ax.get_ylim()[1]\n",
    "ax.set_yticks(range(int(upper_ylimit) + 1)) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d73975-29be-46e0-b0f8-76b968cc17a5",
   "metadata": {},
   "source": [
    "We can also put some part of the date information into a new column, for example if we want to use a Pandas function that only takes column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad95b7-d16c-48fd-84df-8b4496b3f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year'] = data.index.year\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db216c2-de6f-4a84-9c20-838fc6d77761",
   "metadata": {
    "tags": [
     "interlude",
     "pandas",
     "plotting"
    ]
   },
   "source": [
    "<a id=\"boxplots\"></a>\n",
    "###### Interlude: Boxplots\n",
    "Boxplots are a tool often used in statistics to visualize data distributions. Let's try it for our year data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48624c37-129f-4f1e-b2d5-0aa6d35a21dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['PM10', 'PM2.5', 'year']].boxplot(by='year') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e5a66-d278-4bf8-9e4c-44054fcc6b24",
   "metadata": {},
   "source": [
    "Doesn't look that informative and we can hardly see the weird stuff that is drawn at the bottom. This is due to our distribution having a long tail of much higher values. If we limit the maximum measurement value it looks much more useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944372fa-b221-4bfa-8bbd-33f5b8007074",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = data[['PM10', 'PM2.5', 'year']].boxplot(by='year') ;\n",
    "axes[0].set_ylim(0, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f902e0-052f-44ca-be80-f77c23466f15",
   "metadata": {},
   "source": [
    "If you don't know what you are seeing and are interested in it, below is a not so short, but hopefully easy to understand explanation. Otherwise move on to [more boxplots](#per-month) or [another way of plotting summary statistics](#plot-with-table) or if plots are not your thing to [resampling](#resampling).\n",
    "\n",
    "The blue boxes show us three things:\n",
    "* the green line in the middle is the median, so half the values are larger than it and have are smaller.\n",
    "* The end of the box are the 1st and 3rd quartile or 25% and 75% percentile. It is very similar to the median, just that it is not half and half, but 1/4 smaller and 3/4 larger for the 1st quartile and the other way around for the 3rd quartile.\n",
    "\n",
    "All of these values we looked at a couple of times already when we called `describe`. Here they are just visualized to compare the values more easily. The information we can get from the boxes is for example that the data is not distributed evenly, because the green line (median or 50% value) is clearly not in the middle of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773b0ab-45fd-4371-b38a-1a454295798e",
   "metadata": {},
   "source": [
    "The stuff that is sticking out from the box is a bit trickier to understand. They are used to show how far the data reaches. For example, here you see at the lower end they always end at (or very close to) zero.\n",
    "\n",
    "However, you often have values in your data that are far outside the typical range. These are often called outliers. To not give them too much weight in the visualization they are not included in the thing sticking out from the box and instead are drawn separately as circles.\n",
    "\n",
    "At what point they are excluded is actually not always based on the same measure. In Pandas it is if they are more than 1 1/2 times further away from the box then the box is long. Seen another way, it takes more than 150% the space to reach them from where half the values are located, then the space which this half of the values take up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f0d331-d959-492c-9a83-b6110ca404fa",
   "metadata": {},
   "source": [
    "<a id=\"per-month\"></a>\n",
    "#### Over months (seasonality)\n",
    "Variations within a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8af49d-7710-42d4-8cd8-6a875ddd9f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7f0b01-ec0e-4144-9ead-e7aa6a6a5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0, ax1 = data.boxplot(['PM10', 'PM2.5'], by='month')\n",
    "ax0.set_ylim(0, 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96de12-27e3-4569-89fe-2c24e1394abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax0, ax1 = data.boxplot(['PM10', 'PM2.5'], by='month')\n",
    "ax0.set_ylim(0, 80)\n",
    "ax0.axhline(data['PM10'].median(), color='orange')\n",
    "ax1.axhline(data['PM2.5'].median(), color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccd25b-2c04-4552-a627-b751e1329906",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_stat = data['PM2.5'].groupby(data.index.month).describe()\n",
    "month_stat.rename_axis(index='month', inplace=True)\n",
    "month_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9689583-b95e-446d-b2e8-6a3827c922fe",
   "metadata": {},
   "source": [
    "<a id=\"plot-with-table\"></a>\n",
    "##### Plotting summary statistics over month\n",
    "Once more calculating the month statistics if you skipped it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3141d065-7d4a-4214-bb93-fed8df03c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data.index.month\n",
    "month_stat = data['PM2.5'].groupby(data.index.month).describe().\\\n",
    "    rename_axis(index='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c35d25-76ea-4802-bf01-0d5e36e4504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = month_stat.drop(['count', 'min'], axis=1).plot(marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1790ed-f2e0-43f6-8463-03c403783bf6",
   "metadata": {},
   "source": [
    "What a confusing mess. Let's move the `max` data to the right $y$ axis and make it use a different scale (or we just give up on the max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d3dd6-4281-4f21-a57e-28b639f831c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = month_stat.drop(['count', 'min'], axis=1).plot(marker='o',\n",
    "                                                    secondary_y='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c85bf6-9808-43d7-900f-b28a21dd4326",
   "metadata": {},
   "source": [
    "Better, but still hard to read as hell. Maybe now you start to appreciate [boxplots](#boxplots) a bit more?! Although to be fair, we are attempting to visualize a bit more information than in the boxplot (i.e. the mean value and standard deviation).\n",
    "\n",
    "But let's not give up already. Making the curves easy to distinguish from each other could help a lot. Time for styling and a small Python trick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58bfb93-e03b-4f1c-8956-2fdc52fdaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "['a', 'b', 'c'] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12fd2d-233f-46fb-b1a1-d303755d3622",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = month_stat.drop(['count', 'min'], axis=1).plot(\n",
    "    secondary_y='max', marker='o', style=['-', '-.', ':']*3,\n",
    "    colormap='Spectral', figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31fb5ca-fd18-4eeb-89d0-44a64fad2bcd",
   "metadata": {},
   "source": [
    "Better, but still far from great :(\n",
    "\n",
    "For styling advice I recommend to look at the [matplotlib example gallery](https://matplotlib.org/stable/gallery/index.html) and for a listing of [color schemes this example](https://matplotlib.org/stable/gallery/color/colormap_reference.html#sphx-glr-gallery-color-colormap-reference-py) in particular.\n",
    "\n",
    "The yellow is hard to see, but most colormaps have some light-colored component or otherwise they are not distinguishable enough. We'll see later how to do [some more easy styling](#plot-styling), like using a darker background. Marching on, maybe we can communicate the information in a different way:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a8e40-33f6-491a-8c9f-1c7c06ca0d30",
   "metadata": {
    "tags": [
     "interlude",
     "plotting",
     "pandas"
    ]
   },
   "source": [
    "##### Interlude: Adding exact values as tabular data to plot\n",
    "The plot above is overloaded and confusing. Additionally, we don't get to see the exact values. Let's try to fix that step by step.\n",
    "\n",
    "First, we can use the handy `table` parameter of the pandas plot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7f2ac-06be-4846-9f9f-9a3bb169bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = month_stat.drop(['count', 'min'], axis=1).plot(secondary_y='max', marker='o',\n",
    "    style=['-', '-.', ':']*3, colormap='Spectral', figsize=(12, 8),\n",
    "    table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77b962-dac8-4a86-a003-0f46f3ed4be3",
   "metadata": {},
   "source": [
    "To avoid overlapping the table with the $x$ axis ticks and label, we can just get rid of them. The ticks we move to the top and the label we just leave away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae84b5-ec2c-4eed-aa3f-9a40ddee6b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = month_stat.drop(['count', 'min'], axis=1).plot(secondary_y='max', marker='o',\n",
    "    style=['-', '-.', ':']*3, colormap='Spectral', figsize=(12, 8),\n",
    "    table=True)\n",
    "ax.xaxis.tick_top()\n",
    "ax.set_xlabel('') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4181c7f-1df8-4c40-be35-a7c5dc84d88a",
   "metadata": {},
   "source": [
    "The number formatting in the table makes it hard to read. By default, it shows the full precision of the numbers. We can make our own table and round the numbers to the precision we want to have in the plot.\n",
    "\n",
    "Also, month names instead of numbers would be useful. Instead of typing all the month names, we can get the information from the built-in `calendar` module. That module can actually do quite a bit more than that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e650ae0-1585-40af-a705-263b6c1c86c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "month_table = np.round(month_stat, 2).drop('count', axis=1).T\n",
    "month_table.columns = [calendar.month_abbr[int(num)] for num in month_table.columns]\n",
    "month_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b2c0c-cdbf-4269-8bd7-662981a2dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calendar.calendar(2021))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50583a53-21b4-4d6e-86bf-9b8c0c51e3ec",
   "metadata": {},
   "source": [
    "Instead of nicely formatting the calendar and printing it, we can also obtain the dates per week as a list of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfaf06-644c-489e-aece-0b9a44ed1c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.monthcalendar(2021, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a8c6b-6d01-4a73-9f99-b7a42bbbf565",
   "metadata": {},
   "source": [
    "Now we can add our table with the nicer formatting to the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b6b22-ba39-4f92-b5f7-3ef580aa0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = month_stat.drop(['count', 'min'], axis=1).plot(secondary_y='max', marker='o',\n",
    "    style=['-', '-.', ':']*3, colormap='Spectral', figsize=(12, 8),\n",
    "    table=month_table)\n",
    "ax.xaxis.tick_top()  # Display x-axis ticks on top.\n",
    "ax.set_xlabel('') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488800d-e9f0-415f-ab31-cf27bbe530b7",
   "metadata": {},
   "source": [
    "Much better, but the main plot is still visually overloaded. Now that we have the numeric data in the bottom though, we can only display the most important curves.\n",
    "\n",
    "Additionally, we can just show the range of the `25%` to the `75%` percentiles filling the space between with a slight shade instead of displaying the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b818bb8-30b3-4ff8-a5e4-5336c5b781e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = month_stat[['mean', 'std', '50%']].plot(\n",
    "    secondary_y='max', marker='o', style=['-', '-.']*3,\n",
    "    figsize=(12, 8), table=month_table, grid=True)\n",
    "ax.fill_between(month_stat.index, '25%', '75%', data=month_stat,\n",
    "                color='gray', alpha=0.2)\n",
    "ax.xaxis.tick_top()  # Display x-axis ticks on top.\n",
    "ax.set_xlabel('') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d280679-10d4-4335-b6b7-bcbc7158858a",
   "metadata": {},
   "source": [
    "Okay, now it looks actually useable. We could still add labels to describe what the shaded area is and make the are extend the $x$ limits, but let's not overdo it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112cd29-9cd0-4664-8d7f-2ffb0296edba",
   "metadata": {},
   "source": [
    "### Short-term variations\n",
    "#### Day by day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7048b5f-ec21-419b-9476-1d581e5569e2",
   "metadata": {
    "tags": [
     "interlude",
     "pandas"
    ]
   },
   "source": [
    "<a id=\"resampling\"></a>\n",
    "##### Interlude: Resampling the data to a fixed time interval\n",
    "\n",
    "So far, the distance between two consecutive data points varies. This makes our life harder than it needs to be in a couple of ways:\n",
    "* it is harder to access exact time points, as we don't know where they exactly are (if we guess we are likely a few seconds or more off)\n",
    "* combining our data with another data set (e.g. other sensor data or weather data) is much easier if both have a fixed time frequency\n",
    "* Last but not least: Pandas does nicer $x$ label formatting for us\n",
    "\n",
    "Generally, it is quite easy to achieve, we just call `resample` and tell it what time interval we want and then we need to tell it how to combine all the data points in an interval to a single data point, e.g. averaging the values with `mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d0483-4e87-40f4-917e-b73c6ff67d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "daily_mean = data.resample('D').mean()\n",
    "daily_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19101a01-55be-4aea-a801-6aaf36eedf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_mean.loc['2021'].plot(y=['PM10', 'PM2.5'], lw=1, marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef9a598-1bad-4fd6-8b48-ad7eae3c69df",
   "metadata": {
    "tags": [
     "pandas",
     "comparison"
    ]
   },
   "source": [
    "##### Detour: Groupby vs. Resample\n",
    "If the difference and uses cases between the two is clear to you, feel free to jump to the [next section](#highest-days).\n",
    "\n",
    "For resampling to days we could actually also achieve the same thing with [groupby](#groupby), although there are some differences: it takes a bit longer and it doesn't include a frequency information by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88c1cf-90d7-4116-90ee-bde48289e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "daily_mean_groupby = data.groupby(data.index.date).mean()\n",
    "daily_mean_groupby.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a082fa-ecf7-4456-8d7a-d402b8a921fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_mean_groupby.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e95a0-1f00-498e-9b0d-a7b6ab38e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(daily_mean), len(daily_mean_groupby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20be2c-7a4c-46d1-8e88-ea536f177a99",
   "metadata": {},
   "source": [
    "The daily mean from the `resample` contains also entries for the missing dates with the values set to NaN (not a number). This makes it easier for some operations and is what pandas needs for having an index of fixed frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26819d-8db6-4ef9-90c0-562c591069f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(daily_mean.dropna(axis='rows', how='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e5c48-d347-47b9-a723-cdc10a92c072",
   "metadata": {},
   "source": [
    "We can change the daily means from the `groupby` into the same format and frequency information by calling `asfreq`, which adds padding for the missing days and also adds the frequency information to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e61d8-a2ee-4abf-b60d-5c1b279e9fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_mean_groupby.asfreq('D').equals(\n",
    "    daily_mean.set_index(daily_mean.index.date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5b51cd-3a48-46ee-8126-584c8e7643e3",
   "metadata": {},
   "source": [
    "In general, [groupby](#groupby) is used when we have a not too huge number of groups and there doesn't need to be any kind of logical order from one group to another.\n",
    "\n",
    "In contrast [resample](#resample) can only be used for timeseries data with a datetime index. There is a clear temporal ordering between the groups. It is likely faster than groupby, maybe because it makes good use of this ordering information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1294aae6-d772-40b7-8dec-ec8b2a363eb1",
   "metadata": {},
   "source": [
    "<a id=\"highest-days\"></a>\n",
    "##### Days with highest average PM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef63ee38-300d-4fc7-b784-52fa545e9288",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_mean.nlargest(10, 'PM2.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdd20b-8787-47ce-a880-722c464c5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_mean.nlargest(50, 'PM2.5')['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75590c14-c446-4e41-b7f2-8051103af135",
   "metadata": {},
   "source": [
    "One thing you might have noticed is that the New Year's Day and New Year's Eve showed up a couple of times among the highest days. Let's look a bit closer at this, by averaging over shorter time spans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df4f61-45ea-41ad-8fc5-75de6c628621",
   "metadata": {},
   "source": [
    "##### Longer peaks within a day / hourly means resampling\n",
    "From plotting the data in the beginning, we have seen a lot of peaks and some of them are very high. However, most of the peaks have a very short duration. Longer duration peaks are more interesting, as they have a bigger impact on longer term averages (like daily above) and because it is not from a very transient event (or even from a measurement artifact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc67723-7c4d-4776-b6d0-0596a42e2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_mean = data.resample('1h').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec93b3a-22e1-49d1-8dfa-b920c5480bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_mean.nlargest(15, 'PM2.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f49518-cbc6-4e3c-9c52-ea2cbe9598b5",
   "metadata": {},
   "source": [
    "The keen explorer might have noticed that the resample labels are always at the start (or end) of the frequency period and not in the middle of it. For days it does make sense and for hours to some degree as well, though if you plot them, the values lag half an hour behind what you might expect.\n",
    "\n",
    "In a short moment we'll look at [how to center the resample labels](#resample-center)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ff7ad-8802-4473-9045-234e253e00d8",
   "metadata": {},
   "source": [
    "##### Detour: Looking at PM around New Year's Eve\n",
    "*also known as* \"it's not rocket science\"\n",
    "\n",
    "Nearly all of the highest hourly means are actually around New Year's Eve! Let's have a look at it visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c2e4a-3ee6-46e2-a18d-e3cabdbfb8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2018-12-29':'2019-01-03'].plot(y=['PM10', 'PM2.5'])\n",
    "data.loc['2019-12-29':'2020-01-03'].plot(y=['PM10', 'PM2.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00463b-0a4c-4226-8a2f-1a704c54e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2018-12-31':'2019-01-01'].plot(y=['PM10', 'PM2.5'])\n",
    "data.loc['2019-12-31':'2020-01-01'].plot(y=['PM10', 'PM2.5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325aee6-53ee-4732-a75b-e47238be7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc['2018-12-31 12:00':'2019-01-01 12:00'].plot(y=['PM10', 'PM2.5'])\n",
    "data.loc['2019-12-31 12:00':'2020-01-01 12:00'].plot(y=['PM10', 'PM2.5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4429e4-d06b-4a91-b6f2-0973c23b64d5",
   "metadata": {},
   "source": [
    "The spikes are highest just after the midnight, as we would expect with all the rockets going up. If we hadn't fixed the time zone information, latest now we would have noticed it!\n",
    "\n",
    "Let's look even closer, but for that we need to decrease our time interval a bit to say 10min."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3528947-0406-4779-b444-9c3f527d476c",
   "metadata": {
    "tags": [
     "interlude",
     "pandas"
    ]
   },
   "source": [
    "<a id=\"resample-center\"></a>\n",
    "##### Interlude: How to center resample labels\n",
    "\n",
    "One thing to take notice off about `resample` method is that it puts the label at the start or the end of the interval (by default automatically depending on which frequency, e.g. for days at the beginning, but for quarters at the end of it). While we can choose between beginning and end, putting the label in the middle is currently not an option to choose from! :(\n",
    "\n",
    "We can work around this a bit by shifting the time labels by half the interval before resampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ea5ae-f15f-489f-96a5-41fce55ee17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0f0bd-93de-4959-a6a7-6c25138d5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resample('10min').mean().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d56c37-20a6-46fd-ae63-9273216dfe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[:2].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022dc702-79fa-48a9-8a3d-fc7436d5e3b1",
   "metadata": {},
   "source": [
    "So for the first interval, 23:20, it takes from 23:20 - 23:29:59, so here the first two rows, calculates the mean, and then puts the label at the start of the interval, 23:20, instead of the middle, 23:25, as would make more sense in this case.\n",
    "\n",
    "It used to be a bit easier, by using the `loffset` parameter in resample, but that is deprecated and will be removed in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8726a3c-10bb-42f4-aac9-3254e6f3f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resample('10min', loffset='5min').mean().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3a764-a305-4786-81fb-80529f36e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10min = data[['PM10', 'PM2.5']].resample('10min').mean()\n",
    "data_10min.index = data_10min.index + pd.tseries.frequencies.to_offset('5min')\n",
    "data_10min.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d39e20-5229-4621-a437-280de4c4afac",
   "metadata": {
    "tags": [
     "interlude",
     "pandas"
    ]
   },
   "source": [
    "<a id=\"rolling\"></a>\n",
    "###### Mini-Interlude: Rolling average\n",
    "We can smooth it a bit by taking a rolling average with `rolling` and then `mean`. Again, we have to be careful about putting the label in the middle of the rolling interval, but here we can at least achieve this easily by adding `center=True`.\n",
    "\n",
    "Taking rolling averages or other rolling statistics (e.g. `max`) is really easy in Pandas and also quite powerful. For example, we can put different weights on the data points within the rolling window with the `win_type` parameter, like a triangular window or a Gaussian bell curve (and many [more advanced window types](https://docs.scipy.org/doc/scipy/reference/reference/signal.windows.html#module-scipy.signal.windows) from the [SciPy](https://docs.scipy.org/doc/scipy/reference/tutorial/index.html) [signal processing module](https://docs.scipy.org/doc/scipy/reference/reference/signal.html#module-scipy.signal)). We'll make use of that a bit later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70d7cf-87a9-4e3c-934f-038b4ce27b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10min.rolling(3).mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9a401c-ee3d-4fcb-9f3b-29fbc66ebaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10min.rolling(3, center=True).mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27379730-4ab6-4b56-9416-37d2ed684343",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10min.rolling(3, center=True).mean().loc['2018-12-29':'2019-01-03'].plot()\n",
    "data_10min.rolling(3, center=True).mean().loc['2019-12-29':'2020-01-03'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bb845-4d61-4b2d-a798-17087a5fc1f4",
   "metadata": {
    "tags": [
     "interlude",
     "plotting"
    ]
   },
   "source": [
    "<a id=\"subplots\"></a>\n",
    "##### Interlude: Multiple Plots in one figure\n",
    "Having more than one plot in a figure is actually quite easy with matplotlib. You just call `plt.subplots` and tell it how many rows and columns you want to have and it gives you back the figure (the thing that contains the plots) and the axes (what maptlotlib calls the area for one plot). The Pandas plotting libraries accept an axes to plot into with the `ax` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3cb20d-cfec-4393-bfdd-a70e4630666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2)\n",
    "data_10min.rolling(3, center=True).mean().loc[\n",
    "    '2018-12-29':'2019-01-03'].plot(ax=axes[0])\n",
    "data_10min.rolling(3, center=True).mean().loc[\n",
    "    '2019-12-29':'2020-01-03'].plot(ax=axes[1]) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38317c1-8991-41da-b43b-345e54360cb8",
   "metadata": {},
   "source": [
    "With `subplots` you can also specify if the plots have the same $x$ or $y$ axis range with the `sharex` and `sharey` parameters. You can also set the figure size with `figsize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a1332e-3a00-493e-9b7d-c23203a1580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, sharey=True, figsize=(12, 5))\n",
    "data_10min.rolling(3, center=True).mean().loc[\n",
    "    '2018-12-29':'2019-01-03'].plot(ax=axes[0], xlabel='')\n",
    "data_10min.rolling(3, center=True).mean().loc[\n",
    "    '2019-12-29':'2020-01-03'].plot(ax=axes[1], xlabel='') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8482371d-fbb6-4b4f-8f9b-4199604d7e3b",
   "metadata": {},
   "source": [
    "If you have multiple rows and columns you have to be a bit careful with how to access them, as in that case subplots gives you back an array of arrays / list of lists, where the outer list contains the rows and the inner lists the columns within a row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740193b-3bef-446f-a13e-292db25cb0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,0.5))\n",
    "axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c09a64-10c5-483d-8b0a-536d38d044a6",
   "metadata": {},
   "source": [
    "Often you just want to create one subplot after another. There is an easy way to do that by squashing the array of arrays into one longer array with `flatten`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbd7557-74a0-4ec7-a7f5-ce92ac6470cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "axes.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748bc538-373d-42df-a867-ccd478a4a1f7",
   "metadata": {},
   "source": [
    "We can avoid copying the code for each year by looping over it. And then using string formatting with format strings to include the number we want. This is not the nicest way to do this for sure, but we already know how to do it with datetime strings and laziness often wins in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb710e-ed6d-46f3-9e79-a901e15725c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, sharey=True, figsize=(12,12))\n",
    "df = data_10min.rolling(5, center=True, win_type='triang').mean()\n",
    "for ax, year in zip(axes.flatten(), range(17,21)):\n",
    "    df.loc[f'20{year}-12-29':f'20{year+1}-01-03'].plot(ax=ax, xlabel='')\n",
    "ax.set_ylim(0, 200) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c473b03f-ec3a-4838-8553-5dac34e8ca2f",
   "metadata": {},
   "source": [
    "<a id=\"new-year-with-line\"></a>\n",
    "###### New Year's Eve close-up\n",
    "The last and first day of the year. To not miss in miss the year changing we add a line at midnight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c6049-5534-4cd1-9ddf-1fb8b9abc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, sharey=True, figsize=(12,12))\n",
    "df = data_10min.rolling(3, center=True, win_type='triang').mean()\n",
    "for ax, year in zip(axes.flatten(), range(17,21)):\n",
    "    df.loc[f'20{year}-12-31':f'20{year+1}-01-01'].plot(ax=ax, xlabel='')#rot=20)\n",
    "    ax.axvline(f'20{year+1}-01-01 00:00', color='green', alpha=0.5)\n",
    "ax.set_ylim(0, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c5f58-82ce-4fba-ae0a-39e12563c056",
   "metadata": {},
   "source": [
    "Or even closer, looking at just a 24h window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b14527-e353-4db4-90c8-88963992d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, sharey=True, figsize=(12,12))\n",
    "df = data_10min.rolling(3, center=True, win_type='triang').mean()\n",
    "for ax, year in zip(axes.flatten(), range(17,21)):\n",
    "    df.loc[f'20{year}-12-31 18:00':f'20{year+1}-01-01 6:00'].plot(\n",
    "        ax=ax, marker='.', xlabel='')\n",
    "    ax.axvline(f'20{year+1}-01-01 00:00', color='green', alpha=0.5)\n",
    "ax.set_ylim(0, 200) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54afe8bf-ca4f-428c-b641-8d2a476d9d60",
   "metadata": {},
   "source": [
    "#### Within a day / by time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c302ac-2d9a-4d3c-92b9-41a8096a9684",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10min['time'] = data_10min.index.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c31f07-30c6-423c-91c9-bfb6cebc6ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_time = data_10min.groupby('time')\n",
    "by_time.ngroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9f984-2276-44d1-ad51-d7996b9dee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_time.mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c9f3a-a6cc-4cd4-b976-ba8b67a6c04e",
   "metadata": {},
   "source": [
    "The labels are at weird random times. Let's fix this manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df8a7d-8900-4b34-a0e9-d25b9bba5d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = by_time.mean().plot()\n",
    "ax.set_xticks(range(0, 24*60*60+1, 3*60*60)) ; # time is treated like seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860b5db-5a4a-4d97-ac6c-69f8d74f35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stats = by_time.describe()\n",
    "time_stats.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f2b5aa-de54-409b-807a-36a370a7b908",
   "metadata": {
    "tags": [
     "interlude",
     "pandas",
     "hierarchical-index"
    ]
   },
   "source": [
    "<a id=\"hierarchical-column-index-part1\"></a>\n",
    "##### Interlude: Selecting columns in hierarchical indices (part 1)\n",
    "\n",
    "When we just print the `time_stats` (without flipping it around using `.T`), we can see that we have two column name levels. This is called a hierarchical index (or Multilevel Index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3ff58-25fa-43fa-b5ad-9be9f1df6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb41d1d5-12fd-44a9-85a3-a23ee4ed1bd2",
   "metadata": {},
   "source": [
    "If we try to plot the data like we usually do, we get a long scary looking error, telling us the column name 'mean' can't be accessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9fdcfb-3c59-484d-8103-68e833469e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stats.reset_index().plot('time', 'mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2245c7-559b-4ac6-b586-ce7b44cb12fc",
   "metadata": {},
   "source": [
    "If we look at the columns, we can see that it has two levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b7a91-6b06-4bdd-a766-bb8cc28662e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stats.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b017dffd-a65b-4a69-b183-c844a7da46a3",
   "metadata": {},
   "source": [
    "One way to fix the problem is to just specify both column levels with a tuple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b90f4f9-6b68-4395-8d58-5e2f0b6d29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stats.reset_index().plot('time', ('PM2.5','mean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d141c-61ff-4d77-80d7-8f04a2296563",
   "metadata": {},
   "source": [
    "But generally, it is much easier to first select one entry in the upper level and then we can work with it like we are used to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824b875-5c7b-4170-b11f-0605a3c2c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = time_stats['PM2.5'].plot(y=['mean', '50%'])\n",
    "ax.set_xlim(time_stats.index[0], time_stats.index[-1])\n",
    "ax.set_xticks(range(0, 24*60*60+1, 3*60*60))\n",
    "ax.fill_between(time_stats.index, '25%', '75%', data=time_stats['PM2.5'],\n",
    "                color='gray',alpha=0.3) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991554d8-f81f-4ce6-b6eb-e837e631d1f2",
   "metadata": {},
   "source": [
    "We'll see more ways to deal with a hierarchical column index [a bit later](#hierarchical-column-index-part2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f436ec4-c390-4776-901e-653ceb52580f",
   "metadata": {},
   "source": [
    "#### Within a week / by weekday and time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70afdf-bc9b-436b-a9ba-3a39057711cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weekday_stats = data_10min.groupby(data_10min.index.weekday).describe()\n",
    "weekday_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd59c8-ec87-473e-9619-7c5e04a55721",
   "metadata": {},
   "source": [
    "But what do the day numbers mean? According to Pandas documentation the first day of the week by default is Monday and therefore has the value 0, Tuesday 1,...\n",
    "\n",
    "From the calendar module we can get the short versions of the weekday (even localized if we want that). They also start the week with Monday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ad11f-e3eb-41eb-b6b0-92e862155337",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weekday_stats.index = calendar.day_abbr\n",
    "weekday_stats.T # flip the table around to make it easier to read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f69ea-b193-4e4c-a46c-af390779c50a",
   "metadata": {},
   "source": [
    "The PM2.5 daily mean for Monday is by far the lowest. Let's double check we didn't mix up Monday with Sunday:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845e2fb-ef2a-41db-a5aa-7497ea536501",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.to_datetime('2021-06-17')\n",
    "today.day_name(), today.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b5e716-7346-4d3e-9ece-6cedecc56f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_monday = pd.to_datetime('2021-06-14')\n",
    "last_monday.day_name(), last_monday.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456329d-c55e-4c5f-8107-31b0ec22b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10min.groupby(data_10min.index.day_name())['PM2.5'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffde6fab-f433-44b2-afb0-3adda29f0dd5",
   "metadata": {},
   "source": [
    "Seems to be really the case that Monday is lowest for PM2.5. Maybe we can make sense of it by looking at how the pollution changes over the day for each weekday."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9c4157-7428-4d8c-aa17-447de3e7018c",
   "metadata": {
    "tags": [
     "interlude",
     "pandas"
    ]
   },
   "source": [
    "<a id=\"hierarchical-column-index1\"></a>\n",
    "##### Interlude: Grouping by more than one column\n",
    "We can group by more than one value in Pandas. This will give us a hierarchical index (also called Multi Index) for the rows. As we have seen with already in the columns, hierarchical indices can be a bit of a pain to deal with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99ff14-02ad-498d-ad5f-0ee77409c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this actually takes a few seconds\n",
    "by_day_time = data_10min.groupby(\n",
    "    [data_10min.index.weekday, data_10min.index.time])\n",
    "day_time_stats = by_day_time.describe()\n",
    "day_time_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811240a2-f48c-49b3-868c-48dd9e716f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = day_time_stats['PM2.5'].plot(y=['mean', '50%'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50adfdb1-a5e1-4c0b-89be-5d6f9922bdeb",
   "metadata": {},
   "source": [
    "Let's try and figure out how the x-axis values work in the plot with this strange index, by playing around a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c32b54-5508-4107-b415-5cd4a0af1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = day_time_stats['PM2.5'].plot(y=['mean', '50%'])\n",
    "ax.axvline(0)\n",
    "ax.axvline(24)\n",
    "ax.axvline(24*6, color='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe3a3f-725f-4ed5-ae06-e1fa05894767",
   "metadata": {},
   "source": [
    "Looks like the x-value is just the row number. We have 6 entries per hour, so one day is 24*6. Let's make the plot a bit easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b734005-294f-4c8c-8fb7-56e25d84e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant in ['PM2.5', 'PM10']:\n",
    "    ax = day_time_stats[pollutant].rolling(5, center=True).mean().plot(\n",
    "        y=['mean', '50%'], title=pollutant)\n",
    "    ax.set_xlim(0, 7*24*6)\n",
    "    for i in range(1, 8):\n",
    "        ax.axvline(i*24*6, color='gray')\n",
    "    ax.axhline(data[pollutant].mean(), alpha=0.5)\n",
    "    ax.axhline(data[pollutant].median(), color='C1', alpha=0.5)\n",
    "    ax.set_xticks(range(12*6, 7*24*6, 24*6))\n",
    "    ax.set_xticklabels(calendar.day_abbr)\n",
    "    ax.set_xlabel('time of day and week day')\n",
    "    ax.set_ylabel(pollutant) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e24f89-51f8-4d72-963d-21496da33d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_stats.loc[:, (['PM2.5', 'PM10'], ['mean', '50%'])].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace4538c-cfba-499e-ab6f-c69f2c5f2f91",
   "metadata": {
    "tags": [
     "interlude",
     "pandas",
     "hierarchical-index"
    ]
   },
   "source": [
    "<a id=\"hierarchical-column-index-part2\"></a>\n",
    "##### Interlude: Selecting columns in hierarchical indices (part 2)\n",
    "\n",
    "Accessing some value from the lower column index, but all from the upper, is a bit tricky.\n",
    "\n",
    "There are two not too verbose alternatives to do that. Usually we select all entries by using `:`, but in a tuple like `(upper_columns,lower_columns)` we are not allowed to do `(:,lower_column)` as this is an invalid syntax in Python.\n",
    "\n",
    "However, the `:` is just a short form for accessing slices in Python and we can use the more explicit form of `slice(None)` to select all entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bd504-db55-41cd-8d9f-10e1b51c22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_stats.loc[:, (['PM2.5', 'PM10'], ['mean', '50%'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7216c2-43ce-4d0c-a35d-8949626fb59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_stats.loc[:, (slice(None), ['mean', '50%'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee47426-926e-46eb-a98a-c27f974b29ad",
   "metadata": {},
   "source": [
    "Or we just **swap the order of the columns** (temporarily or permanently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e40d6-36ad-4737-8e75-60142a6e3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_stats.swaplevel(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2425b9-5221-4b6b-9e9f-d36d71529979",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_stats.swaplevel(axis='columns')[['mean', '50%']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725dec60-fa6c-45d6-874a-2d39ef55c3e3",
   "metadata": {
    "tags": [
     "interlude",
     "pandas",
     "hierarchical-index"
    ]
   },
   "source": [
    "##### Interlude: Changing a hierarchical row index into a normal index\n",
    "We can also get rid of one layer in a hierarchical index, by removing it from the index. For the row index we can achieve this by calling `reset_index()` with the number or name of the level we want to remove.\n",
    "\n",
    "To plot all the days in one plot with pandas, this form is more useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04eddd-5272-4b4f-ada2-ae292f5b9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_time_stats['PM2.5'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8888ee-c487-46a3-a61c-0f48401337a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_time_stats['PM2.5'].reset_index(level=0).rename(\n",
    "    columns={'timestamp': 'weekday'}).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af08c42-ed99-4753-9ba3-0ea250154891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = day_time_stats['PM2.5'].reset_index(level=0).rename(\n",
    "    columns={'timestamp': 'weekday'})\n",
    "df.groupby('weekday', sort=False)['mean'].plot(legend=True) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5bad7-7d08-4292-85a4-c9d0ace94743",
   "metadata": {},
   "source": [
    "*We could actually do the groupby also with an index level (but we haven't done that yet).*\n",
    "\n",
    "Let's try to make this plot a bit easier to read, by smoothing the data a bit and applying some styling to the plot.\n",
    "\n",
    "We know we are only interested in the PM2.5 mean for now, so let's start with that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb8f8a-3fa2-4be3-b881-28316216b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_time_stats['PM2.5']['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454445e3-407d-4f98-9dd2-18ae2464ba4f",
   "metadata": {},
   "source": [
    "In this case we have two row index level, but no columns at all. It will be easier to work with if we move one level to the columns. We can do this with `unstack` (and we can also move a level of columns to the row index with `stack`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c4826-9881-43b1-8083-850e6c70cdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_time_stats['PM2.5']['mean'].unstack(level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a743756-2e01-4224-8b85-b7bf42f23c90",
   "metadata": {},
   "source": [
    "It is better if we do the smoothing before the unstacking, then we can smooth the whole week instead of each day separately. This way we get less null values because we have only one beginning and end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a3573-ec7b-4205-9dd6-9e317bbf4797",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_by_weekday = day_time_stats['PM2.5']['mean'].rolling(5, center=True).mean()\n",
    "pm25_by_weekday = pm25_by_weekday.unstack(level=0)\n",
    "pm25_by_weekday.columns = calendar.day_abbr\n",
    "pm25_by_weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20226e01-cf4e-434b-ad63-952d3491842c",
   "metadata": {
    "tags": [
     "pandas"
    ]
   },
   "source": [
    "##### Note: even more reshaping with pivot and melt\n",
    "If you didn't get enough of moving index levels around or the methods above don't click for you there is two more common reshaping methods: `pivot` for going from long to wide (creating more columns and less rows) and `melt` for going from wide to long.\n",
    "\n",
    "Some packages, especially ones related to statistics, like [seaborn](https://seaborn.pydata.org/) (a very handy plotting library for more statistical plots) expect \"tidy data\". This generally means a long format (many rows), rather than a wide one. Quick plotting with Pandas or Matplotlib on the other hand is easier with a wide format (a column per line or plot you want to draw). If you start using these other packages you probably need to get a bit familiar with the reshaping operations at some point. It can be quite tricky to wrap your head around it, especially if you use it only occasionally. In case it comforts you, doing the same thing in R was not any easier (at least the last time I used it).\n",
    "\n",
    "I won't go into detail of how these reshaping methods work, as it would be a bit much even for this wild ride. Also, there are a lot of resources out there that explain it better with nice diagrams and all, like the [official Pandas reshaping tutorial](https://pandas.pydata.org/docs/getting_started/intro_tutorials/07_reshape_table_layout.html#long-to-wide-table-format) which, funnily enough, also uses air quality data. In that tutorials the `pivot_table` method is also described, which some of you might be familiar with from Excel or so. It is similar to `pivot`, but instead of just reshaping it also aggregates some of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9863f-011c-4bfb-a629-a9a9803f198d",
   "metadata": {
    "tags": [
     "interlude",
     "plotting",
     "pandas"
    ]
   },
   "source": [
    "<a id=\"plot-styling\"></a>\n",
    "##### Interlude: Plot styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e9007-8c51-471c-840f-9d0d9ba7cf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_by_weekday.plot()\n",
    "plt.xticks(pm25_by_weekday.index[::6*3]) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a8380-cf21-46be-a477-c1fb1a0f6b0a",
   "metadata": {},
   "source": [
    "Still difficult to read. Let's use different line styles with the `style` parameter. Let's use two different line styles for the weekdays and one for the weekend. In Python we can do some handy operations on list to repeat them or concatenate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766f90e-326e-4d20-80ae-ed39a61dabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "['a', 'b'] * 3 + ['x', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca553f47-6216-488b-b163-19be20ffb885",
   "metadata": {},
   "source": [
    "Let's also choose different plot colors. Matplotlib comes with quite a few color palettes, which are [listed and visualized](https://matplotlib.org/stable/gallery/color/colormap_reference.html) in the [Gallery](https://matplotlib.org/stable/gallery/) as one of the examples. In this case we have some order (Mon-Sun), but also a bit of discontinuity between weekdays and weekend. Let's try to find a color map that reflects that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ccb307-ec73-4dc4-a4cf-5b28062d6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_by_weekday.plot(style=(['--',  ':']*3)[:5] + ['-']*2, lw=2.5,\n",
    "                     colormap='summer')\n",
    "plt.xticks(range(0, 24*60*60+1, 3*60*60)) ;\n",
    "plt.xlim(0, 24*60*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e851f-86a5-4fa1-9d56-bbc590344be1",
   "metadata": {},
   "source": [
    "Okayish, but Sunday is hard to read. To improve this, we can use a different matplotlib style for this plot, like one with a dark background, but let's first quickly put the plotting code into a function, so we don't have to copy it all the time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4ea72-7ac3-475a-b32a-7f52a58372db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weekdays_together(ax=None):\n",
    "    pm25_by_weekday.plot(style=(['--',  ':']*3)[:5] + ['-']*2, lw=2.5,\n",
    "                         colormap='summer', ax=ax)\n",
    "    plt.xticks(range(0, 24*60*60+1, 3*60*60))\n",
    "    plt.xlim(0, 24*60*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1f53d-afc9-46b0-8b9e-2117559eb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af17f6-3f00-41e5-bdd2-4b7e2269d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-darkgrid'):\n",
    "    plot_weekdays_together()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab814cc-d119-4b4b-ad6f-64feb4cf8a4b",
   "metadata": {},
   "source": [
    "Or if you don't like not having the box around the dark area (and can't be bothered to add it back, like me), maybe this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5779834-8ce1-4b8b-adfe-7bfce73c46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context(['bmh']):\n",
    "    plot_weekdays_together()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6dfce-f829-4d67-89c9-e40485220296",
   "metadata": {},
   "source": [
    "Or maybe you like it really dark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bed343-4d14-473c-8587-eb8eab7b6a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'):\n",
    "    plot_weekdays_together()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2aa2d-a954-4069-b9cf-d2ed3463ed9b",
   "metadata": {
    "tags": [
     "pandas"
    ]
   },
   "source": [
    "## Getting the results out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c43813-148d-4004-b347-41e481458df5",
   "metadata": {
    "tags": [
     "pandas"
    ]
   },
   "source": [
    "### Sharing the exploration\n",
    "If we want to share all our results with other people, we can just share the Jupyter Notebook itself. In case the audience might not have Python and Jupyter installed (or it would be too much effort to open it that way), you can get the whole notebook as **HTML file** by going to \"File\" -> \"Export Notebook As...\" -> \"HTML\". There are also quite a few possibilities to select what should get included, but many of them are currenlty only accessible from the command line. For example, you can hide all code in the html output with `jupyter nbconvert notebook_name.ipynb --no-input --to html`.\n",
    "\n",
    "There are a lot of other formats there, but HTML is generally the easiest to share, apart from maybe PDF. However, **PDF files** requires LaTeX to be installed (and \"Webpdf\" at least needs additional packages installed). It might require some formatting work as well in the original notebook to ensure e.g. code lines are not too long and get cut off.\n",
    "\n",
    "In case you want to use the code from your notebook in a script or in a larger program, you can export all the code in the notebook with \"File\" -> \"Export Notebook As...\" -> \"Executable Script\". This will give you a **Python file** with all the code, but it also includes the cell numbers and the markdown text cells as comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde2d453-7f77-49d5-a74d-4484d935f329",
   "metadata": {},
   "source": [
    "### Exporting plots\n",
    "To save a plot to a file you can use `plt.savefig` to save the current figure in a large variety of formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b5ff2-1281-4aa2-a3b8-05b55e9a62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weekdays_together()\n",
    "plt.savefig('pm_by_weekday.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e125c57-83ec-4899-819c-b3239584eb16",
   "metadata": {},
   "source": [
    "If you don't add any further arguments, there probably a lot of empty space around the plot. You can change that with setting the `bbox_inches` parameter for example to `'tight'`.\n",
    "\n",
    "If only want to save the plot but not display it in the notebook, you can close the figure in the same cell (but after `savefig`). If you don't have a handle to the figure object, you can get it with `plt.gcf()` (get current figure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22261fd-8d89-4e39-9e2b-8e30fa49949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'):\n",
    "    plot_weekdays_together()\n",
    "plt.savefig('pm_by_weekday.pdf', bbox_inches='tight')\n",
    "plt.close(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02035a-e200-436a-8706-f696fc854593",
   "metadata": {},
   "source": [
    "If you save it as a png or other raster file format, you probably want to increase the `dpi` (dots per inch) a bit, especially if it is for printing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f751e5-ab9a-40c4-b1e2-ba9ae226b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('dark_background'):\n",
    "    plot_weekdays_together()\n",
    "plt.savefig('pm_by_weekday.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bcab8-a437-4c9f-a5e2-eb5a36a35b6b",
   "metadata": {},
   "source": [
    "But if you just quickly want to share the png image with someone else or copy it somewhere, the quality of the plot in the notebook is often enough. You can quickly get that image with `Shift + Right-Click` on the image in the notebook and then choosing something like \"Copy Image\" from your browser menu. Usually you can copy images in the browser by just right-clicking on an image (without pressing `Shift`), but Jupyter Lab uses that for offering you other options, but at least it tells you what to do to get to the browser menu at the very bottom of that options list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c99170-d816-43aa-b91d-13a26a8646c3",
   "metadata": {},
   "source": [
    "### Exporting Pandas dataframes\n",
    "Pandas supports a lot of output formats. There are all the formats you would expect, like CSV, some you might hope for, like Excel, and some that you probably wouldn't expect, like to copy it to clipboard in a format that allows you to paste it into Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ff4de-f5db-448e-b999-e8073b220a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interesting_stats = year_stats['PM2.5'].drop(['count', 'std'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f66e5d-9c6c-4b48-a954-6ee1b76c3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_stats.to_clipboard(excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b257ede-1bf1-4678-9023-17d203768459",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_stats.to_clipboard(excel=False, index_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc07e7f-f16e-42e8-88f1-2f29d33d4e49",
   "metadata": {},
   "source": [
    "Or for web developers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9583b218-0151-4b0a-bcff-c26c1c255dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_stats.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3bf239-6430-470e-9dd7-0d607793ddaa",
   "metadata": {},
   "source": [
    "We can also get the dataframe tables as HTML. The `to_html` output is quite long (as it is nicely formatted), so you might want to collapse the output by clicking on the long blue bar next to the output that you get when you click on the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74ac63a-b702-48e5-942e-b001ef915974",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(interesting_stats.to_html(index_names=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c15f0-7336-4297-8867-c6eb9d55db51",
   "metadata": {},
   "source": [
    "We can copy the HTML output into Jupyter markdown cells as well. Could be quite handy to referring to some data from another notebook, right? This is what you get:\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>count</th>\n",
    "      <th>mean</th>\n",
    "      <th>std</th>\n",
    "      <th>min</th>\n",
    "      <th>25%</th>\n",
    "      <th>50%</th>\n",
    "      <th>75%</th>\n",
    "      <th>max</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>2017</th>\n",
    "      <td>96750.0</td>\n",
    "      <td>6.021118</td>\n",
    "      <td>5.011754</td>\n",
    "      <td>0.3</td>\n",
    "      <td>2.40</td>\n",
    "      <td>4.60</td>\n",
    "      <td>8.27</td>\n",
    "      <td>158.83</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2018</th>\n",
    "      <td>174778.0</td>\n",
    "      <td>8.989901</td>\n",
    "      <td>7.295606</td>\n",
    "      <td>0.3</td>\n",
    "      <td>3.70</td>\n",
    "      <td>7.10</td>\n",
    "      <td>12.60</td>\n",
    "      <td>194.97</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2019</th>\n",
    "      <td>164439.0</td>\n",
    "      <td>8.224983</td>\n",
    "      <td>7.543542</td>\n",
    "      <td>0.3</td>\n",
    "      <td>2.93</td>\n",
    "      <td>6.23</td>\n",
    "      <td>11.27</td>\n",
    "      <td>253.73</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2020</th>\n",
    "      <td>199255.0</td>\n",
    "      <td>7.126150</td>\n",
    "      <td>6.378018</td>\n",
    "      <td>0.3</td>\n",
    "      <td>2.43</td>\n",
    "      <td>4.93</td>\n",
    "      <td>10.40</td>\n",
    "      <td>246.83</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2021</th>\n",
    "      <td>89974.0</td>\n",
    "      <td>6.808873</td>\n",
    "      <td>6.330431</td>\n",
    "      <td>0.3</td>\n",
    "      <td>2.30</td>\n",
    "      <td>5.23</td>\n",
    "      <td>9.83</td>\n",
    "      <td>280.87</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eea604-8923-438d-99a9-c8e4f91fcd1d",
   "metadata": {},
   "source": [
    "Like with all the outputs (apart from the clipboard) we can save it directly to a file. We can also open that file in Jupyter Lab. You'll notice that the HTML file doesn't look that nice outside of a Jupyter Notebook. This is because the styling information (CSS markup) is not part of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe442cbc-9220-4afc-813b-1040be01ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_stats.to_html('interesting_stats.html', index_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d956220-dee2-4c21-b74f-b532b7f3bb49",
   "metadata": {},
   "source": [
    "#### One last goody\n",
    "In case you are a big $\\LaTeX$ lover or even just an occasional user, here is one last goody for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb429b3f-1658-47a9-9f67-da1e70c51edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interesting_stats.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4b9c8-8ed7-4a6d-9ba3-3a67556f2729",
   "metadata": {},
   "source": [
    "Instead of printing you probably want to save it as a tex file though and then include it in your $\\LaTeX$ file with `\\input{name_of_exported.tex}`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093df30-1bae-4794-8fa8-7fd5a06dbfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_table.T.to_latex('interesting_stats.tex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b0be4-34c3-4293-8c1a-f82f650fa6c0",
   "metadata": {},
   "source": [
    "# Closing words\n",
    "## Notebook best practice advice\n",
    "\n",
    "Often when you finish working on a notebook it makes sense to restart the session and executing everything again (automatically or manually) or alternatively right clicking on the notebook name in the file browser in the left toolbar and selecting \"Duplicate\".\n",
    "\n",
    "Starting execution from scratch helps with catching situations like variable names that were changed over time, but the old one was still available and so you didn't notice it or that you accidentally changed the execution order of the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6444675-c144-45c1-a23e-47013b7ec1d5",
   "metadata": {},
   "source": [
    "In case you want to share your notebook with other people or want to turn it into nicer Python code to use in other places, you probably need to go over your notebook and clean it up a bit. For example, if you copied a cell several times and then just made small changes to the original cell, you probably want to extract the code lines and turn it into a proper function. Depending on how big and messy your original notebook is, you could either duplicate it and then do the changes in that copy or start a new notebook and copy the relevant parts over, e.g. by drag and dropping the relevant cells to the new notebook (while both are open in a split view)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5a86d-a958-442c-a27a-0275f60fd40e",
   "metadata": {},
   "source": [
    "## Moving on from here\n",
    "If you made it all the way down here congratulations! I appreciate you taking the time to look and work through something new.\n",
    "\n",
    "But now that you are done, what's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01e8dfb-e3da-4a4e-928b-3b1daf94e8eb",
   "metadata": {},
   "source": [
    "### Goodbye Python?\n",
    "Maybe you'll forget all about Python and Pandas, that's okay. Maybe a time will come up when you happen upon a problem that could be solved well with Python and you remember it then. Or whatever you learned here might help you in another programming language. If you end up doing some data analysis with a different language (like R or Julia), that will probably be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e5772e-5862-4524-bcaa-db7ab5cdaf59",
   "metadata": {},
   "source": [
    "### Hello Notebook?\n",
    "Even if you forget all about anything snake related and never need anything of this again, I hope that the concept of \"computational notebooks\" like this Jupyter Notebook will inspire you a bit. Don't forget that they can also be used with other languages, even some that might surprise you at first (like C++ or C#).\n",
    "\n",
    "I think this way of combining code with data, explanations and visualizations is a great way for exploring something new and also for archiving it. Even if it is some small throw-away exploration and you think you'll never come back to it, you might get surprised. It's quite likely that you'll come back to some earlier parts during the exploration and even afterwards it at least happened to me a few times already that I came back to look at the code or one of the plots because I needed it again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568d813e-30d0-45a4-bb79-670770657602",
   "metadata": {},
   "source": [
    "In my opinion it is also a great for learning and teaching. I'm convinced that without Jupyter Notebooks some of the plentiful and high-quality learning resources and package documentation in the Python world would not exist. And for the learner the ability to just play around a bit with the code, tweak a number here, try something different there, is opening up a whole new world.\n",
    "\n",
    "Even if you are not using any material prepared by others, notebooks (and consoles) can be a great way to figure out how to do something in an area that you are not yet familiar with. With tab completion, access to the help strings and documentation, coupled with googling stuff and finding answers close to your problem, you can start to get into a kind of dialog with the programming language and the data. The notebook can be a recording of that dialog in case you want to retrace some of your steps or reflect on it. Or you can tidy it up a bit and help somebody else make a similar journey. You can also just extract what you learned either as code (by exporting as \"Executable Script\") or maybe just the insight, ideas or algorithms you discovered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4dfe4-19f6-43f5-a479-4e5ac90a7c5c",
   "metadata": {},
   "source": [
    "### Or travelling on?\n",
    "In case you are still or (hopefully) more interested in Python and want to look at it a bit more closely, there are many paths you could take to continue your journey from here.\n",
    "\n",
    "One could be to go through this notebook again, but trying it with a [different sensor](https://maps.sensor.community/), maybe one closer to you, and comparing it with this one, trying to change the code where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa16b3d-9adc-4b1c-a91c-84991b55105b",
   "metadata": {},
   "source": [
    "Maybe you are sick and tired of PM data (it *was* a lot). In that case there are so many resources out there, many of them freely available. Quite a few books are available both as texts to read, as well as Jupyter notebooks to go through (in case you enjoyed it with this one and even otherwise: most of them take you along a much gentler journey)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ba78a-c0f6-4228-ae97-9b0be083e1bf",
   "metadata": {},
   "source": [
    "There is also a ton of talks and [tutorial recordings](https://pyvideo.org/tag/tutorial/) available from hundreds of Python conferences. If you like learning from videos head over to [PyVideo.org](https://pyvideo.org/) where you'll find over 10 000 Videos in English and nearly 200 in German. There is also a real flood of online guides and tutorials. One that stands out a bit in quality and in scope is \"Real Python\", like this [introduction to Pandas dataframes](). They do also have a [list of learning paths](https://realpython.com/learning-paths/) that might inspire or help guide you on your way forward (though be warned, most of their video series have only the first few videos freely available, but all written guides I have seen there were free)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46af652-f1ce-46cc-a3b6-4ad2e0fe300e",
   "metadata": {},
   "source": [
    "At least in the data science space the official tutorials of packages have increased a lot in quality in the last few years. So, don't miss out on looking at their tutorials, even if you are just a casual user. I sure wish I had looked at the start of the [basic concepts of Matplotlib](https://matplotlib.org/stable/tutorials/introductory/usage.html) earlier, as there they describe quite concisely what each part in a figure is called or the two different main styles of using matplotlib. However, these package tutorials tend to assume more familiarity with Python (and often even the package itself) than most books and other online guides, so they are probably not the best starting point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
